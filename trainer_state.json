{
  "best_metric": 0.22086845338344574,
  "best_model_checkpoint": "./RF_MODEL/checkpoint-1400",
  "epoch": 13.461538461538462,
  "eval_steps": 200,
  "global_step": 1400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 2101523.0,
      "learning_rate": 1.9996000000000003e-05,
      "loss": 5.2781,
      "step": 2
    },
    {
      "epoch": 0.04,
      "grad_norm": 2207142.75,
      "learning_rate": 1.9992e-05,
      "loss": 3.5363,
      "step": 4
    },
    {
      "epoch": 0.06,
      "grad_norm": 1371710.625,
      "learning_rate": 1.9988000000000002e-05,
      "loss": 3.0495,
      "step": 6
    },
    {
      "epoch": 0.08,
      "grad_norm": 1463979.875,
      "learning_rate": 1.9984e-05,
      "loss": 1.9848,
      "step": 8
    },
    {
      "epoch": 0.1,
      "grad_norm": 1610761.25,
      "learning_rate": 1.9980000000000002e-05,
      "loss": 1.6785,
      "step": 10
    },
    {
      "epoch": 0.12,
      "grad_norm": 1765978.0,
      "learning_rate": 1.9976000000000003e-05,
      "loss": 1.3094,
      "step": 12
    },
    {
      "epoch": 0.13,
      "grad_norm": 1429748.75,
      "learning_rate": 1.9972e-05,
      "loss": 1.6064,
      "step": 14
    },
    {
      "epoch": 0.15,
      "grad_norm": 823062.1875,
      "learning_rate": 1.9968e-05,
      "loss": 1.2877,
      "step": 16
    },
    {
      "epoch": 0.17,
      "grad_norm": 1138110.875,
      "learning_rate": 1.9964e-05,
      "loss": 1.1544,
      "step": 18
    },
    {
      "epoch": 0.19,
      "grad_norm": 749884.1875,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 1.1706,
      "step": 20
    },
    {
      "epoch": 0.21,
      "grad_norm": 902376.1875,
      "learning_rate": 1.9956000000000003e-05,
      "loss": 1.3337,
      "step": 22
    },
    {
      "epoch": 0.23,
      "grad_norm": 812388.5,
      "learning_rate": 1.9952e-05,
      "loss": 1.2575,
      "step": 24
    },
    {
      "epoch": 0.25,
      "grad_norm": 702952.6875,
      "learning_rate": 1.9948e-05,
      "loss": 0.9893,
      "step": 26
    },
    {
      "epoch": 0.27,
      "grad_norm": 773022.75,
      "learning_rate": 1.9944e-05,
      "loss": 0.8195,
      "step": 28
    },
    {
      "epoch": 0.29,
      "grad_norm": 593934.5625,
      "learning_rate": 1.9940000000000002e-05,
      "loss": 0.9842,
      "step": 30
    },
    {
      "epoch": 0.31,
      "grad_norm": 893969.125,
      "learning_rate": 1.9936000000000004e-05,
      "loss": 0.8984,
      "step": 32
    },
    {
      "epoch": 0.33,
      "grad_norm": 706573.25,
      "learning_rate": 1.9932e-05,
      "loss": 0.7264,
      "step": 34
    },
    {
      "epoch": 0.35,
      "grad_norm": 695761.875,
      "learning_rate": 1.9928e-05,
      "loss": 0.5659,
      "step": 36
    },
    {
      "epoch": 0.37,
      "grad_norm": 728528.8125,
      "learning_rate": 1.9924e-05,
      "loss": 0.7443,
      "step": 38
    },
    {
      "epoch": 0.38,
      "grad_norm": 817748.875,
      "learning_rate": 1.9920000000000002e-05,
      "loss": 0.7985,
      "step": 40
    },
    {
      "epoch": 0.4,
      "grad_norm": 1097015.875,
      "learning_rate": 1.9916e-05,
      "loss": 0.5519,
      "step": 42
    },
    {
      "epoch": 0.42,
      "grad_norm": 716436.25,
      "learning_rate": 1.9912000000000002e-05,
      "loss": 0.5441,
      "step": 44
    },
    {
      "epoch": 0.44,
      "grad_norm": 619850.375,
      "learning_rate": 1.9908e-05,
      "loss": 0.4572,
      "step": 46
    },
    {
      "epoch": 0.46,
      "grad_norm": 693327.6875,
      "learning_rate": 1.9904e-05,
      "loss": 0.5523,
      "step": 48
    },
    {
      "epoch": 0.48,
      "grad_norm": 366170.21875,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.3018,
      "step": 50
    },
    {
      "epoch": 0.5,
      "grad_norm": 651533.5625,
      "learning_rate": 1.9896e-05,
      "loss": 0.5413,
      "step": 52
    },
    {
      "epoch": 0.52,
      "grad_norm": 268670.4375,
      "learning_rate": 1.9892000000000002e-05,
      "loss": 0.3581,
      "step": 54
    },
    {
      "epoch": 0.54,
      "grad_norm": 582495.3125,
      "learning_rate": 1.9888e-05,
      "loss": 0.4434,
      "step": 56
    },
    {
      "epoch": 0.56,
      "grad_norm": 349543.5625,
      "learning_rate": 1.9884e-05,
      "loss": 0.362,
      "step": 58
    },
    {
      "epoch": 0.58,
      "grad_norm": 1415226.25,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 0.7461,
      "step": 60
    },
    {
      "epoch": 0.6,
      "grad_norm": 477183.875,
      "learning_rate": 1.9876e-05,
      "loss": 0.4724,
      "step": 62
    },
    {
      "epoch": 0.62,
      "grad_norm": 642209.8125,
      "learning_rate": 1.9872000000000002e-05,
      "loss": 0.3908,
      "step": 64
    },
    {
      "epoch": 0.63,
      "grad_norm": 417690.96875,
      "learning_rate": 1.9868e-05,
      "loss": 0.3518,
      "step": 66
    },
    {
      "epoch": 0.65,
      "grad_norm": 1103783.875,
      "learning_rate": 1.9864e-05,
      "loss": 0.3242,
      "step": 68
    },
    {
      "epoch": 0.67,
      "grad_norm": 1000782.1875,
      "learning_rate": 1.9860000000000003e-05,
      "loss": 0.7145,
      "step": 70
    },
    {
      "epoch": 0.69,
      "grad_norm": 538868.5,
      "learning_rate": 1.9856e-05,
      "loss": 0.3243,
      "step": 72
    },
    {
      "epoch": 0.71,
      "grad_norm": 372886.90625,
      "learning_rate": 1.9852000000000002e-05,
      "loss": 0.4245,
      "step": 74
    },
    {
      "epoch": 0.73,
      "grad_norm": 676330.5,
      "learning_rate": 1.9848e-05,
      "loss": 0.5026,
      "step": 76
    },
    {
      "epoch": 0.75,
      "grad_norm": 610211.25,
      "learning_rate": 1.9844000000000002e-05,
      "loss": 0.5753,
      "step": 78
    },
    {
      "epoch": 0.77,
      "grad_norm": 600406.375,
      "learning_rate": 1.9840000000000003e-05,
      "loss": 0.4417,
      "step": 80
    },
    {
      "epoch": 0.79,
      "grad_norm": 1249976.25,
      "learning_rate": 1.9836e-05,
      "loss": 0.3656,
      "step": 82
    },
    {
      "epoch": 0.81,
      "grad_norm": 681696.4375,
      "learning_rate": 1.9832000000000003e-05,
      "loss": 0.3034,
      "step": 84
    },
    {
      "epoch": 0.83,
      "grad_norm": 421404.53125,
      "learning_rate": 1.9828e-05,
      "loss": 0.4091,
      "step": 86
    },
    {
      "epoch": 0.85,
      "grad_norm": 621393.4375,
      "learning_rate": 1.9824000000000002e-05,
      "loss": 0.3495,
      "step": 88
    },
    {
      "epoch": 0.87,
      "grad_norm": 508852.84375,
      "learning_rate": 1.982e-05,
      "loss": 0.3228,
      "step": 90
    },
    {
      "epoch": 0.88,
      "grad_norm": 433555.1875,
      "learning_rate": 1.9816e-05,
      "loss": 0.2315,
      "step": 92
    },
    {
      "epoch": 0.9,
      "grad_norm": 582860.6875,
      "learning_rate": 1.9812000000000003e-05,
      "loss": 0.562,
      "step": 94
    },
    {
      "epoch": 0.92,
      "grad_norm": 400222.03125,
      "learning_rate": 1.9808e-05,
      "loss": 0.1704,
      "step": 96
    },
    {
      "epoch": 0.94,
      "grad_norm": 457060.75,
      "learning_rate": 1.9804000000000002e-05,
      "loss": 0.3115,
      "step": 98
    },
    {
      "epoch": 0.96,
      "grad_norm": 784799.25,
      "learning_rate": 1.98e-05,
      "loss": 0.5348,
      "step": 100
    },
    {
      "epoch": 0.98,
      "grad_norm": 602409.25,
      "learning_rate": 1.9796e-05,
      "loss": 0.4094,
      "step": 102
    },
    {
      "epoch": 1.0,
      "grad_norm": 659828.0,
      "learning_rate": 1.9792000000000003e-05,
      "loss": 0.3295,
      "step": 104
    },
    {
      "epoch": 1.02,
      "grad_norm": 197463.140625,
      "learning_rate": 1.9788e-05,
      "loss": 0.1515,
      "step": 106
    },
    {
      "epoch": 1.04,
      "grad_norm": 830347.75,
      "learning_rate": 1.9784000000000002e-05,
      "loss": 0.4115,
      "step": 108
    },
    {
      "epoch": 1.06,
      "grad_norm": 492980.875,
      "learning_rate": 1.978e-05,
      "loss": 0.3722,
      "step": 110
    },
    {
      "epoch": 1.08,
      "grad_norm": 589817.9375,
      "learning_rate": 1.9776000000000002e-05,
      "loss": 0.3549,
      "step": 112
    },
    {
      "epoch": 1.1,
      "grad_norm": 492342.46875,
      "learning_rate": 1.9772000000000003e-05,
      "loss": 0.3388,
      "step": 114
    },
    {
      "epoch": 1.12,
      "grad_norm": 174266.59375,
      "learning_rate": 1.9768e-05,
      "loss": 0.2723,
      "step": 116
    },
    {
      "epoch": 1.13,
      "grad_norm": 814619.0625,
      "learning_rate": 1.9764000000000003e-05,
      "loss": 0.3897,
      "step": 118
    },
    {
      "epoch": 1.15,
      "grad_norm": 639206.4375,
      "learning_rate": 1.976e-05,
      "loss": 0.3791,
      "step": 120
    },
    {
      "epoch": 1.17,
      "grad_norm": 755696.5,
      "learning_rate": 1.9756000000000002e-05,
      "loss": 0.4149,
      "step": 122
    },
    {
      "epoch": 1.19,
      "grad_norm": 220796.703125,
      "learning_rate": 1.9752000000000003e-05,
      "loss": 0.2164,
      "step": 124
    },
    {
      "epoch": 1.21,
      "grad_norm": 1839018.125,
      "learning_rate": 1.9748e-05,
      "loss": 0.4173,
      "step": 126
    },
    {
      "epoch": 1.23,
      "grad_norm": 863054.1875,
      "learning_rate": 1.9744e-05,
      "loss": 0.2272,
      "step": 128
    },
    {
      "epoch": 1.25,
      "grad_norm": 399550.21875,
      "learning_rate": 1.974e-05,
      "loss": 0.5618,
      "step": 130
    },
    {
      "epoch": 1.27,
      "grad_norm": 538025.1875,
      "learning_rate": 1.9736000000000002e-05,
      "loss": 0.2572,
      "step": 132
    },
    {
      "epoch": 1.29,
      "grad_norm": 247551.203125,
      "learning_rate": 1.9732000000000004e-05,
      "loss": 0.1801,
      "step": 134
    },
    {
      "epoch": 1.31,
      "grad_norm": 478720.71875,
      "learning_rate": 1.9728e-05,
      "loss": 0.1734,
      "step": 136
    },
    {
      "epoch": 1.33,
      "grad_norm": 261445.1875,
      "learning_rate": 1.9724e-05,
      "loss": 0.2866,
      "step": 138
    },
    {
      "epoch": 1.35,
      "grad_norm": 831673.8125,
      "learning_rate": 1.972e-05,
      "loss": 0.4512,
      "step": 140
    },
    {
      "epoch": 1.37,
      "grad_norm": 4338148.0,
      "learning_rate": 1.9716000000000002e-05,
      "loss": 0.3378,
      "step": 142
    },
    {
      "epoch": 1.38,
      "grad_norm": 757750.5,
      "learning_rate": 1.9712000000000004e-05,
      "loss": 0.3448,
      "step": 144
    },
    {
      "epoch": 1.4,
      "grad_norm": 739791.5,
      "learning_rate": 1.9708000000000002e-05,
      "loss": 0.5104,
      "step": 146
    },
    {
      "epoch": 1.42,
      "grad_norm": 684879.0625,
      "learning_rate": 1.9704e-05,
      "loss": 0.3987,
      "step": 148
    },
    {
      "epoch": 1.44,
      "grad_norm": 319762.3125,
      "learning_rate": 1.97e-05,
      "loss": 0.2521,
      "step": 150
    },
    {
      "epoch": 1.46,
      "grad_norm": 866583.5,
      "learning_rate": 1.9696000000000003e-05,
      "loss": 0.2976,
      "step": 152
    },
    {
      "epoch": 1.48,
      "grad_norm": 439762.59375,
      "learning_rate": 1.9692000000000004e-05,
      "loss": 0.2449,
      "step": 154
    },
    {
      "epoch": 1.5,
      "grad_norm": 229518.78125,
      "learning_rate": 1.9688000000000002e-05,
      "loss": 0.2698,
      "step": 156
    },
    {
      "epoch": 1.52,
      "grad_norm": 251896.140625,
      "learning_rate": 1.9684e-05,
      "loss": 0.2579,
      "step": 158
    },
    {
      "epoch": 1.54,
      "grad_norm": 336372.84375,
      "learning_rate": 1.968e-05,
      "loss": 0.1944,
      "step": 160
    },
    {
      "epoch": 1.56,
      "grad_norm": 407458.6875,
      "learning_rate": 1.9676000000000003e-05,
      "loss": 0.1959,
      "step": 162
    },
    {
      "epoch": 1.58,
      "grad_norm": 1273902.5,
      "learning_rate": 1.9672e-05,
      "loss": 0.4137,
      "step": 164
    },
    {
      "epoch": 1.6,
      "grad_norm": 1030843.875,
      "learning_rate": 1.9668000000000002e-05,
      "loss": 0.2912,
      "step": 166
    },
    {
      "epoch": 1.62,
      "grad_norm": 267375.5,
      "learning_rate": 1.9664e-05,
      "loss": 0.1897,
      "step": 168
    },
    {
      "epoch": 1.63,
      "grad_norm": 1202155.625,
      "learning_rate": 1.966e-05,
      "loss": 0.2719,
      "step": 170
    },
    {
      "epoch": 1.65,
      "grad_norm": 571189.875,
      "learning_rate": 1.9656000000000003e-05,
      "loss": 0.226,
      "step": 172
    },
    {
      "epoch": 1.67,
      "grad_norm": 350894.625,
      "learning_rate": 1.9652e-05,
      "loss": 0.2832,
      "step": 174
    },
    {
      "epoch": 1.69,
      "grad_norm": 1175150.625,
      "learning_rate": 1.9648000000000002e-05,
      "loss": 0.4273,
      "step": 176
    },
    {
      "epoch": 1.71,
      "grad_norm": 418224.28125,
      "learning_rate": 1.9644e-05,
      "loss": 0.1824,
      "step": 178
    },
    {
      "epoch": 1.73,
      "grad_norm": 420346.5,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 0.2731,
      "step": 180
    },
    {
      "epoch": 1.75,
      "grad_norm": 222061.671875,
      "learning_rate": 1.9636000000000003e-05,
      "loss": 0.1671,
      "step": 182
    },
    {
      "epoch": 1.77,
      "grad_norm": 400026.40625,
      "learning_rate": 1.9632e-05,
      "loss": 0.2378,
      "step": 184
    },
    {
      "epoch": 1.79,
      "grad_norm": 805906.125,
      "learning_rate": 1.9628000000000002e-05,
      "loss": 0.2527,
      "step": 186
    },
    {
      "epoch": 1.81,
      "grad_norm": 221735.90625,
      "learning_rate": 1.9624e-05,
      "loss": 0.2184,
      "step": 188
    },
    {
      "epoch": 1.83,
      "grad_norm": 414472.59375,
      "learning_rate": 1.9620000000000002e-05,
      "loss": 0.2945,
      "step": 190
    },
    {
      "epoch": 1.85,
      "grad_norm": 460001.71875,
      "learning_rate": 1.9616000000000003e-05,
      "loss": 0.2083,
      "step": 192
    },
    {
      "epoch": 1.87,
      "grad_norm": 235140.875,
      "learning_rate": 1.9612e-05,
      "loss": 0.2037,
      "step": 194
    },
    {
      "epoch": 1.88,
      "grad_norm": 1065662.875,
      "learning_rate": 1.9608000000000003e-05,
      "loss": 0.3281,
      "step": 196
    },
    {
      "epoch": 1.9,
      "grad_norm": 323641.15625,
      "learning_rate": 1.9604e-05,
      "loss": 0.1457,
      "step": 198
    },
    {
      "epoch": 1.92,
      "grad_norm": 182807.453125,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.1719,
      "step": 200
    },
    {
      "epoch": 1.92,
      "eval_loss": 0.30423641204833984,
      "eval_runtime": 15.7336,
      "eval_samples_per_second": 15.0,
      "eval_steps_per_second": 1.907,
      "step": 200
    },
    {
      "epoch": 1.94,
      "grad_norm": 214491.34375,
      "learning_rate": 1.9596e-05,
      "loss": 0.2148,
      "step": 202
    },
    {
      "epoch": 1.96,
      "grad_norm": 431094.84375,
      "learning_rate": 1.9592e-05,
      "loss": 0.1991,
      "step": 204
    },
    {
      "epoch": 1.98,
      "grad_norm": 172557.171875,
      "learning_rate": 1.9588000000000003e-05,
      "loss": 0.2084,
      "step": 206
    },
    {
      "epoch": 2.0,
      "grad_norm": 1870040.0,
      "learning_rate": 1.9584e-05,
      "loss": 0.4302,
      "step": 208
    },
    {
      "epoch": 2.02,
      "grad_norm": 371836.75,
      "learning_rate": 1.9580000000000002e-05,
      "loss": 0.2255,
      "step": 210
    },
    {
      "epoch": 2.04,
      "grad_norm": 262955.9375,
      "learning_rate": 1.9576e-05,
      "loss": 0.1746,
      "step": 212
    },
    {
      "epoch": 2.06,
      "grad_norm": 414159.59375,
      "learning_rate": 1.9572e-05,
      "loss": 0.1574,
      "step": 214
    },
    {
      "epoch": 2.08,
      "grad_norm": 265241.96875,
      "learning_rate": 1.9568000000000003e-05,
      "loss": 0.1564,
      "step": 216
    },
    {
      "epoch": 2.1,
      "grad_norm": 885425.5625,
      "learning_rate": 1.9564e-05,
      "loss": 0.2888,
      "step": 218
    },
    {
      "epoch": 2.12,
      "grad_norm": 398559.84375,
      "learning_rate": 1.9560000000000002e-05,
      "loss": 0.122,
      "step": 220
    },
    {
      "epoch": 2.13,
      "grad_norm": 220803.796875,
      "learning_rate": 1.9556e-05,
      "loss": 0.1359,
      "step": 222
    },
    {
      "epoch": 2.15,
      "grad_norm": 159334.46875,
      "learning_rate": 1.9552000000000002e-05,
      "loss": 0.1427,
      "step": 224
    },
    {
      "epoch": 2.17,
      "grad_norm": 209403.984375,
      "learning_rate": 1.9548000000000003e-05,
      "loss": 0.2456,
      "step": 226
    },
    {
      "epoch": 2.19,
      "grad_norm": 498137.78125,
      "learning_rate": 1.9544e-05,
      "loss": 0.1656,
      "step": 228
    },
    {
      "epoch": 2.21,
      "grad_norm": 504919.96875,
      "learning_rate": 1.9540000000000003e-05,
      "loss": 0.2123,
      "step": 230
    },
    {
      "epoch": 2.23,
      "grad_norm": 395391.09375,
      "learning_rate": 1.9536e-05,
      "loss": 0.3878,
      "step": 232
    },
    {
      "epoch": 2.25,
      "grad_norm": 851775.3125,
      "learning_rate": 1.9532000000000002e-05,
      "loss": 0.2593,
      "step": 234
    },
    {
      "epoch": 2.27,
      "grad_norm": 431703.25,
      "learning_rate": 1.9528000000000003e-05,
      "loss": 0.1854,
      "step": 236
    },
    {
      "epoch": 2.29,
      "grad_norm": 208708.796875,
      "learning_rate": 1.9524e-05,
      "loss": 0.2423,
      "step": 238
    },
    {
      "epoch": 2.31,
      "grad_norm": 232554.65625,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.1102,
      "step": 240
    },
    {
      "epoch": 2.33,
      "grad_norm": 289079.1875,
      "learning_rate": 1.9516e-05,
      "loss": 0.2333,
      "step": 242
    },
    {
      "epoch": 2.35,
      "grad_norm": 784177.625,
      "learning_rate": 1.9512000000000002e-05,
      "loss": 0.2442,
      "step": 244
    },
    {
      "epoch": 2.37,
      "grad_norm": 1744900.625,
      "learning_rate": 1.9508000000000004e-05,
      "loss": 0.1734,
      "step": 246
    },
    {
      "epoch": 2.38,
      "grad_norm": 524384.1875,
      "learning_rate": 1.9504e-05,
      "loss": 0.325,
      "step": 248
    },
    {
      "epoch": 2.4,
      "grad_norm": 586729.3125,
      "learning_rate": 1.95e-05,
      "loss": 0.2201,
      "step": 250
    },
    {
      "epoch": 2.42,
      "grad_norm": 967513.0625,
      "learning_rate": 1.9496e-05,
      "loss": 0.5274,
      "step": 252
    },
    {
      "epoch": 2.44,
      "grad_norm": 183319.609375,
      "learning_rate": 1.9492000000000002e-05,
      "loss": 0.164,
      "step": 254
    },
    {
      "epoch": 2.46,
      "grad_norm": 215978.46875,
      "learning_rate": 1.9488000000000004e-05,
      "loss": 0.1248,
      "step": 256
    },
    {
      "epoch": 2.48,
      "grad_norm": 292482.4375,
      "learning_rate": 1.9484000000000002e-05,
      "loss": 0.326,
      "step": 258
    },
    {
      "epoch": 2.5,
      "grad_norm": 216970.421875,
      "learning_rate": 1.948e-05,
      "loss": 0.1792,
      "step": 260
    },
    {
      "epoch": 2.52,
      "grad_norm": 283818.3125,
      "learning_rate": 1.9476e-05,
      "loss": 0.1786,
      "step": 262
    },
    {
      "epoch": 2.54,
      "grad_norm": 742311.6875,
      "learning_rate": 1.9472000000000003e-05,
      "loss": 0.2844,
      "step": 264
    },
    {
      "epoch": 2.56,
      "grad_norm": 201192.4375,
      "learning_rate": 1.9468000000000004e-05,
      "loss": 0.2222,
      "step": 266
    },
    {
      "epoch": 2.58,
      "grad_norm": 124170.1796875,
      "learning_rate": 1.9464000000000002e-05,
      "loss": 0.1472,
      "step": 268
    },
    {
      "epoch": 2.6,
      "grad_norm": 217015.71875,
      "learning_rate": 1.946e-05,
      "loss": 0.1436,
      "step": 270
    },
    {
      "epoch": 2.62,
      "grad_norm": 306287.25,
      "learning_rate": 1.9456e-05,
      "loss": 0.1363,
      "step": 272
    },
    {
      "epoch": 2.63,
      "grad_norm": 138142.90625,
      "learning_rate": 1.9452000000000003e-05,
      "loss": 0.125,
      "step": 274
    },
    {
      "epoch": 2.65,
      "grad_norm": 470212.0,
      "learning_rate": 1.9448e-05,
      "loss": 0.1684,
      "step": 276
    },
    {
      "epoch": 2.67,
      "grad_norm": 424304.34375,
      "learning_rate": 1.9444000000000002e-05,
      "loss": 0.2257,
      "step": 278
    },
    {
      "epoch": 2.69,
      "grad_norm": 465500.5,
      "learning_rate": 1.944e-05,
      "loss": 0.1587,
      "step": 280
    },
    {
      "epoch": 2.71,
      "grad_norm": 582446.0,
      "learning_rate": 1.9436e-05,
      "loss": 0.2548,
      "step": 282
    },
    {
      "epoch": 2.73,
      "grad_norm": 255612.3125,
      "learning_rate": 1.9432000000000003e-05,
      "loss": 0.1452,
      "step": 284
    },
    {
      "epoch": 2.75,
      "grad_norm": 402322.4375,
      "learning_rate": 1.9428e-05,
      "loss": 0.2866,
      "step": 286
    },
    {
      "epoch": 2.77,
      "grad_norm": 169912.328125,
      "learning_rate": 1.9424e-05,
      "loss": 0.1426,
      "step": 288
    },
    {
      "epoch": 2.79,
      "grad_norm": 96845.2421875,
      "learning_rate": 1.942e-05,
      "loss": 0.3521,
      "step": 290
    },
    {
      "epoch": 2.81,
      "grad_norm": 223138.71875,
      "learning_rate": 1.9416000000000002e-05,
      "loss": 0.116,
      "step": 292
    },
    {
      "epoch": 2.83,
      "grad_norm": 197627.46875,
      "learning_rate": 1.9412000000000003e-05,
      "loss": 0.1415,
      "step": 294
    },
    {
      "epoch": 2.85,
      "grad_norm": 549258.375,
      "learning_rate": 1.9408e-05,
      "loss": 0.1983,
      "step": 296
    },
    {
      "epoch": 2.87,
      "grad_norm": 192827.03125,
      "learning_rate": 1.9404e-05,
      "loss": 0.1246,
      "step": 298
    },
    {
      "epoch": 2.88,
      "grad_norm": 328412.0,
      "learning_rate": 1.94e-05,
      "loss": 0.1581,
      "step": 300
    },
    {
      "epoch": 2.9,
      "grad_norm": 252314.140625,
      "learning_rate": 1.9396000000000002e-05,
      "loss": 0.2428,
      "step": 302
    },
    {
      "epoch": 2.92,
      "grad_norm": 444172.90625,
      "learning_rate": 1.9392000000000003e-05,
      "loss": 0.2469,
      "step": 304
    },
    {
      "epoch": 2.94,
      "grad_norm": 300663.90625,
      "learning_rate": 1.9388e-05,
      "loss": 0.2186,
      "step": 306
    },
    {
      "epoch": 2.96,
      "grad_norm": 348800.625,
      "learning_rate": 1.9384e-05,
      "loss": 0.1595,
      "step": 308
    },
    {
      "epoch": 2.98,
      "grad_norm": 115901.5546875,
      "learning_rate": 1.938e-05,
      "loss": 0.15,
      "step": 310
    },
    {
      "epoch": 3.0,
      "grad_norm": 847146.375,
      "learning_rate": 1.9376000000000002e-05,
      "loss": 0.2911,
      "step": 312
    },
    {
      "epoch": 3.02,
      "grad_norm": 330574.375,
      "learning_rate": 1.9372000000000004e-05,
      "loss": 0.1822,
      "step": 314
    },
    {
      "epoch": 3.04,
      "grad_norm": 261639.953125,
      "learning_rate": 1.9368e-05,
      "loss": 0.1181,
      "step": 316
    },
    {
      "epoch": 3.06,
      "grad_norm": 201244.015625,
      "learning_rate": 1.9364e-05,
      "loss": 0.1125,
      "step": 318
    },
    {
      "epoch": 3.08,
      "grad_norm": 312943.34375,
      "learning_rate": 1.936e-05,
      "loss": 0.2516,
      "step": 320
    },
    {
      "epoch": 3.1,
      "grad_norm": 237798.65625,
      "learning_rate": 1.9356000000000002e-05,
      "loss": 0.1361,
      "step": 322
    },
    {
      "epoch": 3.12,
      "grad_norm": 185663.484375,
      "learning_rate": 1.9352e-05,
      "loss": 0.1381,
      "step": 324
    },
    {
      "epoch": 3.13,
      "grad_norm": 203453.453125,
      "learning_rate": 1.9348000000000002e-05,
      "loss": 0.1323,
      "step": 326
    },
    {
      "epoch": 3.15,
      "grad_norm": 296872.75,
      "learning_rate": 1.9344e-05,
      "loss": 0.1877,
      "step": 328
    },
    {
      "epoch": 3.17,
      "grad_norm": 227732.375,
      "learning_rate": 1.934e-05,
      "loss": 0.1441,
      "step": 330
    },
    {
      "epoch": 3.19,
      "grad_norm": 179039.96875,
      "learning_rate": 1.9336000000000003e-05,
      "loss": 0.1268,
      "step": 332
    },
    {
      "epoch": 3.21,
      "grad_norm": 255345.359375,
      "learning_rate": 1.9332e-05,
      "loss": 0.1377,
      "step": 334
    },
    {
      "epoch": 3.23,
      "grad_norm": 258031.3125,
      "learning_rate": 1.9328000000000002e-05,
      "loss": 0.2237,
      "step": 336
    },
    {
      "epoch": 3.25,
      "grad_norm": 126668.9296875,
      "learning_rate": 1.9324e-05,
      "loss": 0.1212,
      "step": 338
    },
    {
      "epoch": 3.27,
      "grad_norm": 291666.875,
      "learning_rate": 1.932e-05,
      "loss": 0.1396,
      "step": 340
    },
    {
      "epoch": 3.29,
      "grad_norm": 258800.734375,
      "learning_rate": 1.9316000000000003e-05,
      "loss": 0.1533,
      "step": 342
    },
    {
      "epoch": 3.31,
      "grad_norm": 72700.375,
      "learning_rate": 1.9312e-05,
      "loss": 0.2231,
      "step": 344
    },
    {
      "epoch": 3.33,
      "grad_norm": 233717.640625,
      "learning_rate": 1.9308000000000002e-05,
      "loss": 0.2511,
      "step": 346
    },
    {
      "epoch": 3.35,
      "grad_norm": 103247.0,
      "learning_rate": 1.9304e-05,
      "loss": 0.082,
      "step": 348
    },
    {
      "epoch": 3.37,
      "grad_norm": 196626.546875,
      "learning_rate": 1.93e-05,
      "loss": 0.2422,
      "step": 350
    },
    {
      "epoch": 3.38,
      "grad_norm": 248832.59375,
      "learning_rate": 1.9296000000000003e-05,
      "loss": 0.1323,
      "step": 352
    },
    {
      "epoch": 3.4,
      "grad_norm": 341595.03125,
      "learning_rate": 1.9292e-05,
      "loss": 0.2987,
      "step": 354
    },
    {
      "epoch": 3.42,
      "grad_norm": 323785.21875,
      "learning_rate": 1.9288000000000002e-05,
      "loss": 0.1343,
      "step": 356
    },
    {
      "epoch": 3.44,
      "grad_norm": 358370.3125,
      "learning_rate": 1.9284e-05,
      "loss": 0.1344,
      "step": 358
    },
    {
      "epoch": 3.46,
      "grad_norm": 304921.59375,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 0.1983,
      "step": 360
    },
    {
      "epoch": 3.48,
      "grad_norm": 326303.0,
      "learning_rate": 1.9276e-05,
      "loss": 0.1654,
      "step": 362
    },
    {
      "epoch": 3.5,
      "grad_norm": 479261.40625,
      "learning_rate": 1.9272e-05,
      "loss": 0.1482,
      "step": 364
    },
    {
      "epoch": 3.52,
      "grad_norm": 288379.40625,
      "learning_rate": 1.9268000000000003e-05,
      "loss": 0.2945,
      "step": 366
    },
    {
      "epoch": 3.54,
      "grad_norm": 604177.9375,
      "learning_rate": 1.9264e-05,
      "loss": 0.2217,
      "step": 368
    },
    {
      "epoch": 3.56,
      "grad_norm": 259147.296875,
      "learning_rate": 1.9260000000000002e-05,
      "loss": 0.2014,
      "step": 370
    },
    {
      "epoch": 3.58,
      "grad_norm": 206512.234375,
      "learning_rate": 1.9256e-05,
      "loss": 0.1232,
      "step": 372
    },
    {
      "epoch": 3.6,
      "grad_norm": 118706.96875,
      "learning_rate": 1.9252e-05,
      "loss": 0.1195,
      "step": 374
    },
    {
      "epoch": 3.62,
      "grad_norm": 439169.46875,
      "learning_rate": 1.9248000000000003e-05,
      "loss": 0.2161,
      "step": 376
    },
    {
      "epoch": 3.63,
      "grad_norm": 233242.625,
      "learning_rate": 1.9244000000000004e-05,
      "loss": 0.1597,
      "step": 378
    },
    {
      "epoch": 3.65,
      "grad_norm": 167803.234375,
      "learning_rate": 1.9240000000000002e-05,
      "loss": 0.1531,
      "step": 380
    },
    {
      "epoch": 3.67,
      "grad_norm": 274991.0625,
      "learning_rate": 1.9236e-05,
      "loss": 0.1701,
      "step": 382
    },
    {
      "epoch": 3.69,
      "grad_norm": 272099.5625,
      "learning_rate": 1.9232e-05,
      "loss": 0.2,
      "step": 384
    },
    {
      "epoch": 3.71,
      "grad_norm": 594550.5,
      "learning_rate": 1.9228000000000003e-05,
      "loss": 0.2666,
      "step": 386
    },
    {
      "epoch": 3.73,
      "grad_norm": 238903.34375,
      "learning_rate": 1.9224000000000004e-05,
      "loss": 0.1889,
      "step": 388
    },
    {
      "epoch": 3.75,
      "grad_norm": 207646.515625,
      "learning_rate": 1.9220000000000002e-05,
      "loss": 0.1607,
      "step": 390
    },
    {
      "epoch": 3.77,
      "grad_norm": 298557.5,
      "learning_rate": 1.9216e-05,
      "loss": 0.1695,
      "step": 392
    },
    {
      "epoch": 3.79,
      "grad_norm": 538354.3125,
      "learning_rate": 1.9212000000000002e-05,
      "loss": 0.361,
      "step": 394
    },
    {
      "epoch": 3.81,
      "grad_norm": 166764.265625,
      "learning_rate": 1.9208000000000003e-05,
      "loss": 0.1174,
      "step": 396
    },
    {
      "epoch": 3.83,
      "grad_norm": 215336.78125,
      "learning_rate": 1.9204e-05,
      "loss": 0.1114,
      "step": 398
    },
    {
      "epoch": 3.85,
      "grad_norm": 343073.28125,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.2124,
      "step": 400
    },
    {
      "epoch": 3.85,
      "eval_loss": 0.2537629306316376,
      "eval_runtime": 16.6869,
      "eval_samples_per_second": 14.143,
      "eval_steps_per_second": 1.798,
      "step": 400
    },
    {
      "epoch": 3.87,
      "grad_norm": 175453.9375,
      "learning_rate": 1.9196e-05,
      "loss": 0.1118,
      "step": 402
    },
    {
      "epoch": 3.88,
      "grad_norm": 308630.53125,
      "learning_rate": 1.9192000000000002e-05,
      "loss": 0.1698,
      "step": 404
    },
    {
      "epoch": 3.9,
      "grad_norm": 305322.75,
      "learning_rate": 1.9188000000000003e-05,
      "loss": 0.1403,
      "step": 406
    },
    {
      "epoch": 3.92,
      "grad_norm": 195290.515625,
      "learning_rate": 1.9184e-05,
      "loss": 0.1057,
      "step": 408
    },
    {
      "epoch": 3.94,
      "grad_norm": 375787.59375,
      "learning_rate": 1.918e-05,
      "loss": 0.2153,
      "step": 410
    },
    {
      "epoch": 3.96,
      "grad_norm": 153869.015625,
      "learning_rate": 1.9176e-05,
      "loss": 0.1279,
      "step": 412
    },
    {
      "epoch": 3.98,
      "grad_norm": 1216514.625,
      "learning_rate": 1.9172000000000002e-05,
      "loss": 0.1812,
      "step": 414
    },
    {
      "epoch": 4.0,
      "grad_norm": 1019971.625,
      "learning_rate": 1.9168000000000004e-05,
      "loss": 0.2816,
      "step": 416
    },
    {
      "epoch": 4.02,
      "grad_norm": 312110.84375,
      "learning_rate": 1.9164e-05,
      "loss": 0.3112,
      "step": 418
    },
    {
      "epoch": 4.04,
      "grad_norm": 206440.546875,
      "learning_rate": 1.916e-05,
      "loss": 0.3443,
      "step": 420
    },
    {
      "epoch": 4.06,
      "grad_norm": 304646.3125,
      "learning_rate": 1.9156e-05,
      "loss": 0.1647,
      "step": 422
    },
    {
      "epoch": 4.08,
      "grad_norm": 306788.75,
      "learning_rate": 1.9152000000000002e-05,
      "loss": 0.1694,
      "step": 424
    },
    {
      "epoch": 4.1,
      "grad_norm": 234485.6875,
      "learning_rate": 1.9148000000000004e-05,
      "loss": 0.1307,
      "step": 426
    },
    {
      "epoch": 4.12,
      "grad_norm": 429504.21875,
      "learning_rate": 1.9144000000000002e-05,
      "loss": 0.1388,
      "step": 428
    },
    {
      "epoch": 4.13,
      "grad_norm": 192552.15625,
      "learning_rate": 1.914e-05,
      "loss": 0.2148,
      "step": 430
    },
    {
      "epoch": 4.15,
      "grad_norm": 85899.40625,
      "learning_rate": 1.9136e-05,
      "loss": 0.0963,
      "step": 432
    },
    {
      "epoch": 4.17,
      "grad_norm": 585931.125,
      "learning_rate": 1.9132000000000002e-05,
      "loss": 0.1758,
      "step": 434
    },
    {
      "epoch": 4.19,
      "grad_norm": 72456.1171875,
      "learning_rate": 1.9128e-05,
      "loss": 0.0704,
      "step": 436
    },
    {
      "epoch": 4.21,
      "grad_norm": 465472.71875,
      "learning_rate": 1.9124000000000002e-05,
      "loss": 0.2,
      "step": 438
    },
    {
      "epoch": 4.23,
      "grad_norm": 344238.75,
      "learning_rate": 1.912e-05,
      "loss": 0.1489,
      "step": 440
    },
    {
      "epoch": 4.25,
      "grad_norm": 739075.1875,
      "learning_rate": 1.9116e-05,
      "loss": 0.2451,
      "step": 442
    },
    {
      "epoch": 4.27,
      "grad_norm": 256749.75,
      "learning_rate": 1.9112000000000003e-05,
      "loss": 0.128,
      "step": 444
    },
    {
      "epoch": 4.29,
      "grad_norm": 269035.0625,
      "learning_rate": 1.9108e-05,
      "loss": 0.1804,
      "step": 446
    },
    {
      "epoch": 4.31,
      "grad_norm": 100707.859375,
      "learning_rate": 1.9104000000000002e-05,
      "loss": 0.1099,
      "step": 448
    },
    {
      "epoch": 4.33,
      "grad_norm": 252596.53125,
      "learning_rate": 1.91e-05,
      "loss": 0.1507,
      "step": 450
    },
    {
      "epoch": 4.35,
      "grad_norm": 67636.8359375,
      "learning_rate": 1.9096e-05,
      "loss": 0.0855,
      "step": 452
    },
    {
      "epoch": 4.37,
      "grad_norm": 228400.5625,
      "learning_rate": 1.9092000000000003e-05,
      "loss": 0.1101,
      "step": 454
    },
    {
      "epoch": 4.38,
      "grad_norm": 368486.625,
      "learning_rate": 1.9088e-05,
      "loss": 0.1478,
      "step": 456
    },
    {
      "epoch": 4.4,
      "grad_norm": 172048.5,
      "learning_rate": 1.9084000000000002e-05,
      "loss": 0.1362,
      "step": 458
    },
    {
      "epoch": 4.42,
      "grad_norm": 236971.5,
      "learning_rate": 1.908e-05,
      "loss": 0.1511,
      "step": 460
    },
    {
      "epoch": 4.44,
      "grad_norm": 177836.46875,
      "learning_rate": 1.9076e-05,
      "loss": 0.1323,
      "step": 462
    },
    {
      "epoch": 4.46,
      "grad_norm": 331705.625,
      "learning_rate": 1.9072000000000003e-05,
      "loss": 0.1918,
      "step": 464
    },
    {
      "epoch": 4.48,
      "grad_norm": 188521.34375,
      "learning_rate": 1.9068e-05,
      "loss": 0.2203,
      "step": 466
    },
    {
      "epoch": 4.5,
      "grad_norm": 134573.5,
      "learning_rate": 1.9064000000000002e-05,
      "loss": 0.1094,
      "step": 468
    },
    {
      "epoch": 4.52,
      "grad_norm": 355058.875,
      "learning_rate": 1.906e-05,
      "loss": 0.1358,
      "step": 470
    },
    {
      "epoch": 4.54,
      "grad_norm": 585633.5625,
      "learning_rate": 1.9056000000000002e-05,
      "loss": 0.303,
      "step": 472
    },
    {
      "epoch": 4.56,
      "grad_norm": 71530.8046875,
      "learning_rate": 1.9052000000000003e-05,
      "loss": 0.1688,
      "step": 474
    },
    {
      "epoch": 4.58,
      "grad_norm": 321769.875,
      "learning_rate": 1.9048e-05,
      "loss": 0.1498,
      "step": 476
    },
    {
      "epoch": 4.6,
      "grad_norm": 360733.40625,
      "learning_rate": 1.9044000000000003e-05,
      "loss": 0.1963,
      "step": 478
    },
    {
      "epoch": 4.62,
      "grad_norm": 134293.5,
      "learning_rate": 1.904e-05,
      "loss": 0.1407,
      "step": 480
    },
    {
      "epoch": 4.63,
      "grad_norm": 166948.21875,
      "learning_rate": 1.9036000000000002e-05,
      "loss": 0.1265,
      "step": 482
    },
    {
      "epoch": 4.65,
      "grad_norm": 333548.09375,
      "learning_rate": 1.9032e-05,
      "loss": 0.1622,
      "step": 484
    },
    {
      "epoch": 4.67,
      "grad_norm": 199703.28125,
      "learning_rate": 1.9028e-05,
      "loss": 0.1556,
      "step": 486
    },
    {
      "epoch": 4.69,
      "grad_norm": 116657.9140625,
      "learning_rate": 1.9024000000000003e-05,
      "loss": 0.0836,
      "step": 488
    },
    {
      "epoch": 4.71,
      "grad_norm": 153441.015625,
      "learning_rate": 1.902e-05,
      "loss": 0.175,
      "step": 490
    },
    {
      "epoch": 4.73,
      "grad_norm": 172469.484375,
      "learning_rate": 1.9016000000000002e-05,
      "loss": 0.1345,
      "step": 492
    },
    {
      "epoch": 4.75,
      "grad_norm": 538994.3125,
      "learning_rate": 1.9012e-05,
      "loss": 0.2939,
      "step": 494
    },
    {
      "epoch": 4.77,
      "grad_norm": 352421.875,
      "learning_rate": 1.9008e-05,
      "loss": 0.114,
      "step": 496
    },
    {
      "epoch": 4.79,
      "grad_norm": 129330.2890625,
      "learning_rate": 1.9004000000000003e-05,
      "loss": 0.1358,
      "step": 498
    },
    {
      "epoch": 4.81,
      "grad_norm": 276109.375,
      "learning_rate": 1.9e-05,
      "loss": 0.1774,
      "step": 500
    },
    {
      "epoch": 4.83,
      "grad_norm": 271909.09375,
      "learning_rate": 1.8996000000000002e-05,
      "loss": 0.1512,
      "step": 502
    },
    {
      "epoch": 4.85,
      "grad_norm": 132391.078125,
      "learning_rate": 1.8992e-05,
      "loss": 0.1022,
      "step": 504
    },
    {
      "epoch": 4.87,
      "grad_norm": 959039.5,
      "learning_rate": 1.8988000000000002e-05,
      "loss": 0.195,
      "step": 506
    },
    {
      "epoch": 4.88,
      "grad_norm": 235786.40625,
      "learning_rate": 1.8984000000000003e-05,
      "loss": 0.1444,
      "step": 508
    },
    {
      "epoch": 4.9,
      "grad_norm": 172707.3125,
      "learning_rate": 1.898e-05,
      "loss": 0.1314,
      "step": 510
    },
    {
      "epoch": 4.92,
      "grad_norm": 265290.1875,
      "learning_rate": 1.8976000000000003e-05,
      "loss": 0.1492,
      "step": 512
    },
    {
      "epoch": 4.94,
      "grad_norm": 259184.90625,
      "learning_rate": 1.8972e-05,
      "loss": 0.1521,
      "step": 514
    },
    {
      "epoch": 4.96,
      "grad_norm": 515654.0,
      "learning_rate": 1.8968000000000002e-05,
      "loss": 0.1414,
      "step": 516
    },
    {
      "epoch": 4.98,
      "grad_norm": 276611.8125,
      "learning_rate": 1.8964000000000003e-05,
      "loss": 0.1254,
      "step": 518
    },
    {
      "epoch": 5.0,
      "grad_norm": 585783.75,
      "learning_rate": 1.896e-05,
      "loss": 0.2842,
      "step": 520
    },
    {
      "epoch": 5.02,
      "grad_norm": 712530.875,
      "learning_rate": 1.8956e-05,
      "loss": 0.3454,
      "step": 522
    },
    {
      "epoch": 5.04,
      "grad_norm": 303748.8125,
      "learning_rate": 1.8952e-05,
      "loss": 0.2115,
      "step": 524
    },
    {
      "epoch": 5.06,
      "grad_norm": 115839.65625,
      "learning_rate": 1.8948000000000002e-05,
      "loss": 0.147,
      "step": 526
    },
    {
      "epoch": 5.08,
      "grad_norm": 64603.296875,
      "learning_rate": 1.8944000000000004e-05,
      "loss": 0.0889,
      "step": 528
    },
    {
      "epoch": 5.1,
      "grad_norm": 124164.3203125,
      "learning_rate": 1.894e-05,
      "loss": 0.2002,
      "step": 530
    },
    {
      "epoch": 5.12,
      "grad_norm": 345052.21875,
      "learning_rate": 1.8936e-05,
      "loss": 0.1348,
      "step": 532
    },
    {
      "epoch": 5.13,
      "grad_norm": 327851.90625,
      "learning_rate": 1.8932e-05,
      "loss": 0.1706,
      "step": 534
    },
    {
      "epoch": 5.15,
      "grad_norm": 188680.765625,
      "learning_rate": 1.8928000000000002e-05,
      "loss": 0.1023,
      "step": 536
    },
    {
      "epoch": 5.17,
      "grad_norm": 149238.5625,
      "learning_rate": 1.8924000000000004e-05,
      "loss": 0.1132,
      "step": 538
    },
    {
      "epoch": 5.19,
      "grad_norm": 365392.125,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 0.161,
      "step": 540
    },
    {
      "epoch": 5.21,
      "grad_norm": 115047.8671875,
      "learning_rate": 1.8916e-05,
      "loss": 0.0768,
      "step": 542
    },
    {
      "epoch": 5.23,
      "grad_norm": 350813.40625,
      "learning_rate": 1.8912e-05,
      "loss": 0.1648,
      "step": 544
    },
    {
      "epoch": 5.25,
      "grad_norm": 306414.15625,
      "learning_rate": 1.8908000000000003e-05,
      "loss": 0.1411,
      "step": 546
    },
    {
      "epoch": 5.27,
      "grad_norm": 197459.109375,
      "learning_rate": 1.8904000000000004e-05,
      "loss": 0.1434,
      "step": 548
    },
    {
      "epoch": 5.29,
      "grad_norm": 332522.65625,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.1236,
      "step": 550
    },
    {
      "epoch": 5.31,
      "grad_norm": 225235.0625,
      "learning_rate": 1.8896e-05,
      "loss": 0.1294,
      "step": 552
    },
    {
      "epoch": 5.33,
      "grad_norm": 237093.890625,
      "learning_rate": 1.8892e-05,
      "loss": 0.0952,
      "step": 554
    },
    {
      "epoch": 5.35,
      "grad_norm": 113252.6015625,
      "learning_rate": 1.8888000000000003e-05,
      "loss": 0.1227,
      "step": 556
    },
    {
      "epoch": 5.37,
      "grad_norm": 114536.6328125,
      "learning_rate": 1.8884e-05,
      "loss": 0.1073,
      "step": 558
    },
    {
      "epoch": 5.38,
      "grad_norm": 127063.2265625,
      "learning_rate": 1.8880000000000002e-05,
      "loss": 0.1795,
      "step": 560
    },
    {
      "epoch": 5.4,
      "grad_norm": 92854.53125,
      "learning_rate": 1.8876e-05,
      "loss": 0.118,
      "step": 562
    },
    {
      "epoch": 5.42,
      "grad_norm": 1126307.0,
      "learning_rate": 1.8872e-05,
      "loss": 0.1321,
      "step": 564
    },
    {
      "epoch": 5.44,
      "grad_norm": 241947.53125,
      "learning_rate": 1.8868000000000003e-05,
      "loss": 0.3079,
      "step": 566
    },
    {
      "epoch": 5.46,
      "grad_norm": 279354.4375,
      "learning_rate": 1.8864e-05,
      "loss": 0.1023,
      "step": 568
    },
    {
      "epoch": 5.48,
      "grad_norm": 85881.625,
      "learning_rate": 1.886e-05,
      "loss": 0.1272,
      "step": 570
    },
    {
      "epoch": 5.5,
      "grad_norm": 167659.328125,
      "learning_rate": 1.8856e-05,
      "loss": 0.2583,
      "step": 572
    },
    {
      "epoch": 5.52,
      "grad_norm": 236787.296875,
      "learning_rate": 1.8852000000000002e-05,
      "loss": 0.1862,
      "step": 574
    },
    {
      "epoch": 5.54,
      "grad_norm": 274769.84375,
      "learning_rate": 1.8848000000000003e-05,
      "loss": 0.1683,
      "step": 576
    },
    {
      "epoch": 5.56,
      "grad_norm": 191761.34375,
      "learning_rate": 1.8844e-05,
      "loss": 0.1162,
      "step": 578
    },
    {
      "epoch": 5.58,
      "grad_norm": 189116.203125,
      "learning_rate": 1.884e-05,
      "loss": 0.1229,
      "step": 580
    },
    {
      "epoch": 5.6,
      "grad_norm": 180731.546875,
      "learning_rate": 1.8836e-05,
      "loss": 0.1305,
      "step": 582
    },
    {
      "epoch": 5.62,
      "grad_norm": 157629.1875,
      "learning_rate": 1.8832000000000002e-05,
      "loss": 0.1114,
      "step": 584
    },
    {
      "epoch": 5.63,
      "grad_norm": 162182.609375,
      "learning_rate": 1.8828000000000003e-05,
      "loss": 0.1492,
      "step": 586
    },
    {
      "epoch": 5.65,
      "grad_norm": 285318.96875,
      "learning_rate": 1.8824e-05,
      "loss": 0.1324,
      "step": 588
    },
    {
      "epoch": 5.67,
      "grad_norm": 845477.5625,
      "learning_rate": 1.882e-05,
      "loss": 0.1718,
      "step": 590
    },
    {
      "epoch": 5.69,
      "grad_norm": 164870.15625,
      "learning_rate": 1.8816e-05,
      "loss": 0.1369,
      "step": 592
    },
    {
      "epoch": 5.71,
      "grad_norm": 134886.59375,
      "learning_rate": 1.8812000000000002e-05,
      "loss": 0.0958,
      "step": 594
    },
    {
      "epoch": 5.73,
      "grad_norm": 148106.71875,
      "learning_rate": 1.8808e-05,
      "loss": 0.1356,
      "step": 596
    },
    {
      "epoch": 5.75,
      "grad_norm": 124627.7421875,
      "learning_rate": 1.8804e-05,
      "loss": 0.1044,
      "step": 598
    },
    {
      "epoch": 5.77,
      "grad_norm": 125762.0546875,
      "learning_rate": 1.88e-05,
      "loss": 0.1314,
      "step": 600
    },
    {
      "epoch": 5.77,
      "eval_loss": 0.2364526093006134,
      "eval_runtime": 15.4756,
      "eval_samples_per_second": 15.25,
      "eval_steps_per_second": 1.939,
      "step": 600
    },
    {
      "epoch": 5.79,
      "grad_norm": 114869.0546875,
      "learning_rate": 1.8796e-05,
      "loss": 0.0971,
      "step": 602
    },
    {
      "epoch": 5.81,
      "grad_norm": 95279.0859375,
      "learning_rate": 1.8792000000000002e-05,
      "loss": 0.2724,
      "step": 604
    },
    {
      "epoch": 5.83,
      "grad_norm": 165450.71875,
      "learning_rate": 1.8788e-05,
      "loss": 0.1358,
      "step": 606
    },
    {
      "epoch": 5.85,
      "grad_norm": 91862.1328125,
      "learning_rate": 1.8784000000000002e-05,
      "loss": 0.1573,
      "step": 608
    },
    {
      "epoch": 5.87,
      "grad_norm": 139161.71875,
      "learning_rate": 1.878e-05,
      "loss": 0.1036,
      "step": 610
    },
    {
      "epoch": 5.88,
      "grad_norm": 94744.3203125,
      "learning_rate": 1.8776e-05,
      "loss": 0.1253,
      "step": 612
    },
    {
      "epoch": 5.9,
      "grad_norm": 586974.4375,
      "learning_rate": 1.8772000000000003e-05,
      "loss": 0.1845,
      "step": 614
    },
    {
      "epoch": 5.92,
      "grad_norm": 249140.109375,
      "learning_rate": 1.8768e-05,
      "loss": 0.1381,
      "step": 616
    },
    {
      "epoch": 5.94,
      "grad_norm": 178316.453125,
      "learning_rate": 1.8764000000000002e-05,
      "loss": 0.137,
      "step": 618
    },
    {
      "epoch": 5.96,
      "grad_norm": 568486.6875,
      "learning_rate": 1.876e-05,
      "loss": 0.1646,
      "step": 620
    },
    {
      "epoch": 5.98,
      "grad_norm": 84027.5625,
      "learning_rate": 1.8756e-05,
      "loss": 0.0828,
      "step": 622
    },
    {
      "epoch": 6.0,
      "grad_norm": 120814.0,
      "learning_rate": 1.8752000000000003e-05,
      "loss": 0.1564,
      "step": 624
    },
    {
      "epoch": 6.02,
      "grad_norm": 463898.8125,
      "learning_rate": 1.8748e-05,
      "loss": 0.1831,
      "step": 626
    },
    {
      "epoch": 6.04,
      "grad_norm": 261495.765625,
      "learning_rate": 1.8744000000000002e-05,
      "loss": 0.1466,
      "step": 628
    },
    {
      "epoch": 6.06,
      "grad_norm": 111138.0390625,
      "learning_rate": 1.8740000000000004e-05,
      "loss": 0.1185,
      "step": 630
    },
    {
      "epoch": 6.08,
      "grad_norm": 139970.984375,
      "learning_rate": 1.8736e-05,
      "loss": 0.1409,
      "step": 632
    },
    {
      "epoch": 6.1,
      "grad_norm": 92732.078125,
      "learning_rate": 1.8732000000000003e-05,
      "loss": 0.118,
      "step": 634
    },
    {
      "epoch": 6.12,
      "grad_norm": 92885.453125,
      "learning_rate": 1.8728e-05,
      "loss": 0.2592,
      "step": 636
    },
    {
      "epoch": 6.13,
      "grad_norm": 241752.90625,
      "learning_rate": 1.8724000000000002e-05,
      "loss": 0.1067,
      "step": 638
    },
    {
      "epoch": 6.15,
      "grad_norm": 179747.53125,
      "learning_rate": 1.8720000000000004e-05,
      "loss": 0.1444,
      "step": 640
    },
    {
      "epoch": 6.17,
      "grad_norm": 92987.0390625,
      "learning_rate": 1.8716000000000002e-05,
      "loss": 0.0947,
      "step": 642
    },
    {
      "epoch": 6.19,
      "grad_norm": 194814.078125,
      "learning_rate": 1.8712e-05,
      "loss": 0.1332,
      "step": 644
    },
    {
      "epoch": 6.21,
      "grad_norm": 339389.65625,
      "learning_rate": 1.8708e-05,
      "loss": 0.1636,
      "step": 646
    },
    {
      "epoch": 6.23,
      "grad_norm": 161834.984375,
      "learning_rate": 1.8704000000000003e-05,
      "loss": 0.0965,
      "step": 648
    },
    {
      "epoch": 6.25,
      "grad_norm": 82624.4765625,
      "learning_rate": 1.8700000000000004e-05,
      "loss": 0.1038,
      "step": 650
    },
    {
      "epoch": 6.27,
      "grad_norm": 154248.875,
      "learning_rate": 1.8696000000000002e-05,
      "loss": 0.1123,
      "step": 652
    },
    {
      "epoch": 6.29,
      "grad_norm": 108732.609375,
      "learning_rate": 1.8692e-05,
      "loss": 0.1002,
      "step": 654
    },
    {
      "epoch": 6.31,
      "grad_norm": 95532.6328125,
      "learning_rate": 1.8688e-05,
      "loss": 0.1181,
      "step": 656
    },
    {
      "epoch": 6.33,
      "grad_norm": 186725.015625,
      "learning_rate": 1.8684000000000003e-05,
      "loss": 0.144,
      "step": 658
    },
    {
      "epoch": 6.35,
      "grad_norm": 156595.0625,
      "learning_rate": 1.8680000000000004e-05,
      "loss": 0.1559,
      "step": 660
    },
    {
      "epoch": 6.37,
      "grad_norm": 135849.265625,
      "learning_rate": 1.8676000000000002e-05,
      "loss": 0.0876,
      "step": 662
    },
    {
      "epoch": 6.38,
      "grad_norm": 558730.0625,
      "learning_rate": 1.8672e-05,
      "loss": 0.1123,
      "step": 664
    },
    {
      "epoch": 6.4,
      "grad_norm": 152290.625,
      "learning_rate": 1.8668e-05,
      "loss": 0.1394,
      "step": 666
    },
    {
      "epoch": 6.42,
      "grad_norm": 151804.28125,
      "learning_rate": 1.8664000000000003e-05,
      "loss": 0.1296,
      "step": 668
    },
    {
      "epoch": 6.44,
      "grad_norm": 99148.84375,
      "learning_rate": 1.866e-05,
      "loss": 0.1367,
      "step": 670
    },
    {
      "epoch": 6.46,
      "grad_norm": 106837.984375,
      "learning_rate": 1.8656000000000002e-05,
      "loss": 0.1261,
      "step": 672
    },
    {
      "epoch": 6.48,
      "grad_norm": 216200.1875,
      "learning_rate": 1.8652e-05,
      "loss": 0.1226,
      "step": 674
    },
    {
      "epoch": 6.5,
      "grad_norm": 144207.734375,
      "learning_rate": 1.8648000000000002e-05,
      "loss": 0.1453,
      "step": 676
    },
    {
      "epoch": 6.52,
      "grad_norm": 163981.3125,
      "learning_rate": 1.8644000000000003e-05,
      "loss": 0.1214,
      "step": 678
    },
    {
      "epoch": 6.54,
      "grad_norm": 84738.25,
      "learning_rate": 1.864e-05,
      "loss": 0.1839,
      "step": 680
    },
    {
      "epoch": 6.56,
      "grad_norm": 1894075.125,
      "learning_rate": 1.8636e-05,
      "loss": 0.4896,
      "step": 682
    },
    {
      "epoch": 6.58,
      "grad_norm": 151213.828125,
      "learning_rate": 1.8632e-05,
      "loss": 0.1485,
      "step": 684
    },
    {
      "epoch": 6.6,
      "grad_norm": 394208.84375,
      "learning_rate": 1.8628000000000002e-05,
      "loss": 0.1663,
      "step": 686
    },
    {
      "epoch": 6.62,
      "grad_norm": 162612.859375,
      "learning_rate": 1.8624000000000003e-05,
      "loss": 0.1135,
      "step": 688
    },
    {
      "epoch": 6.63,
      "grad_norm": 145856.625,
      "learning_rate": 1.862e-05,
      "loss": 0.1588,
      "step": 690
    },
    {
      "epoch": 6.65,
      "grad_norm": 246267.375,
      "learning_rate": 1.8616e-05,
      "loss": 0.1354,
      "step": 692
    },
    {
      "epoch": 6.67,
      "grad_norm": 109609.03125,
      "learning_rate": 1.8612e-05,
      "loss": 0.3029,
      "step": 694
    },
    {
      "epoch": 6.69,
      "grad_norm": 75077.8515625,
      "learning_rate": 1.8608000000000002e-05,
      "loss": 0.0967,
      "step": 696
    },
    {
      "epoch": 6.71,
      "grad_norm": 136365.0625,
      "learning_rate": 1.8604000000000003e-05,
      "loss": 0.0742,
      "step": 698
    },
    {
      "epoch": 6.73,
      "grad_norm": 393669.25,
      "learning_rate": 1.86e-05,
      "loss": 0.2391,
      "step": 700
    },
    {
      "epoch": 6.75,
      "grad_norm": 184752.828125,
      "learning_rate": 1.8596e-05,
      "loss": 0.1296,
      "step": 702
    },
    {
      "epoch": 6.77,
      "grad_norm": 359813.65625,
      "learning_rate": 1.8592e-05,
      "loss": 0.1698,
      "step": 704
    },
    {
      "epoch": 6.79,
      "grad_norm": 171018.28125,
      "learning_rate": 1.8588000000000002e-05,
      "loss": 0.1459,
      "step": 706
    },
    {
      "epoch": 6.81,
      "grad_norm": 542116.4375,
      "learning_rate": 1.8584000000000004e-05,
      "loss": 0.1461,
      "step": 708
    },
    {
      "epoch": 6.83,
      "grad_norm": 161257.328125,
      "learning_rate": 1.858e-05,
      "loss": 0.1076,
      "step": 710
    },
    {
      "epoch": 6.85,
      "grad_norm": 165739.3125,
      "learning_rate": 1.8576e-05,
      "loss": 0.2592,
      "step": 712
    },
    {
      "epoch": 6.87,
      "grad_norm": 264368.6875,
      "learning_rate": 1.8572e-05,
      "loss": 0.1212,
      "step": 714
    },
    {
      "epoch": 6.88,
      "grad_norm": 271418.9375,
      "learning_rate": 1.8568000000000002e-05,
      "loss": 0.1055,
      "step": 716
    },
    {
      "epoch": 6.9,
      "grad_norm": 200388.65625,
      "learning_rate": 1.8564e-05,
      "loss": 0.1883,
      "step": 718
    },
    {
      "epoch": 6.92,
      "grad_norm": 159741.734375,
      "learning_rate": 1.8560000000000002e-05,
      "loss": 0.1702,
      "step": 720
    },
    {
      "epoch": 6.94,
      "grad_norm": 124802.7734375,
      "learning_rate": 1.8556e-05,
      "loss": 0.1118,
      "step": 722
    },
    {
      "epoch": 6.96,
      "grad_norm": 142656.4375,
      "learning_rate": 1.8552e-05,
      "loss": 0.1226,
      "step": 724
    },
    {
      "epoch": 6.98,
      "grad_norm": 163030.96875,
      "learning_rate": 1.8548000000000003e-05,
      "loss": 0.103,
      "step": 726
    },
    {
      "epoch": 7.0,
      "grad_norm": 334564.8125,
      "learning_rate": 1.8544e-05,
      "loss": 0.1414,
      "step": 728
    },
    {
      "epoch": 7.02,
      "grad_norm": 867412.1875,
      "learning_rate": 1.8540000000000002e-05,
      "loss": 0.2461,
      "step": 730
    },
    {
      "epoch": 7.04,
      "grad_norm": 269350.34375,
      "learning_rate": 1.8536e-05,
      "loss": 0.0946,
      "step": 732
    },
    {
      "epoch": 7.06,
      "grad_norm": 493240.09375,
      "learning_rate": 1.8532e-05,
      "loss": 0.2234,
      "step": 734
    },
    {
      "epoch": 7.08,
      "grad_norm": 174940.765625,
      "learning_rate": 1.8528000000000003e-05,
      "loss": 0.1392,
      "step": 736
    },
    {
      "epoch": 7.1,
      "grad_norm": 511680.25,
      "learning_rate": 1.8524e-05,
      "loss": 0.2208,
      "step": 738
    },
    {
      "epoch": 7.12,
      "grad_norm": 92439.9765625,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 0.112,
      "step": 740
    },
    {
      "epoch": 7.13,
      "grad_norm": 110968.5703125,
      "learning_rate": 1.8516e-05,
      "loss": 0.1127,
      "step": 742
    },
    {
      "epoch": 7.15,
      "grad_norm": 366870.71875,
      "learning_rate": 1.8512e-05,
      "loss": 0.1603,
      "step": 744
    },
    {
      "epoch": 7.17,
      "grad_norm": 1395795.875,
      "learning_rate": 1.8508000000000003e-05,
      "loss": 0.15,
      "step": 746
    },
    {
      "epoch": 7.19,
      "grad_norm": 162643.765625,
      "learning_rate": 1.8504e-05,
      "loss": 0.1068,
      "step": 748
    },
    {
      "epoch": 7.21,
      "grad_norm": 173205.09375,
      "learning_rate": 1.8500000000000002e-05,
      "loss": 0.1143,
      "step": 750
    },
    {
      "epoch": 7.23,
      "grad_norm": 193536.078125,
      "learning_rate": 1.8496e-05,
      "loss": 0.1038,
      "step": 752
    },
    {
      "epoch": 7.25,
      "grad_norm": 373583.875,
      "learning_rate": 1.8492000000000002e-05,
      "loss": 0.1549,
      "step": 754
    },
    {
      "epoch": 7.27,
      "grad_norm": 357650.90625,
      "learning_rate": 1.8488e-05,
      "loss": 0.2053,
      "step": 756
    },
    {
      "epoch": 7.29,
      "grad_norm": 188533.671875,
      "learning_rate": 1.8484e-05,
      "loss": 0.109,
      "step": 758
    },
    {
      "epoch": 7.31,
      "grad_norm": 182777.375,
      "learning_rate": 1.8480000000000003e-05,
      "loss": 0.0995,
      "step": 760
    },
    {
      "epoch": 7.33,
      "grad_norm": 140048.28125,
      "learning_rate": 1.8476e-05,
      "loss": 0.1083,
      "step": 762
    },
    {
      "epoch": 7.35,
      "grad_norm": 122083.71875,
      "learning_rate": 1.8472000000000002e-05,
      "loss": 0.1046,
      "step": 764
    },
    {
      "epoch": 7.37,
      "grad_norm": 244111.84375,
      "learning_rate": 1.8468e-05,
      "loss": 0.1881,
      "step": 766
    },
    {
      "epoch": 7.38,
      "grad_norm": 110533.2109375,
      "learning_rate": 1.8464e-05,
      "loss": 0.166,
      "step": 768
    },
    {
      "epoch": 7.4,
      "grad_norm": 108470.59375,
      "learning_rate": 1.8460000000000003e-05,
      "loss": 0.1102,
      "step": 770
    },
    {
      "epoch": 7.42,
      "grad_norm": 130830.890625,
      "learning_rate": 1.8456e-05,
      "loss": 0.0969,
      "step": 772
    },
    {
      "epoch": 7.44,
      "grad_norm": 442680.75,
      "learning_rate": 1.8452000000000002e-05,
      "loss": 0.2261,
      "step": 774
    },
    {
      "epoch": 7.46,
      "grad_norm": 153844.53125,
      "learning_rate": 1.8448e-05,
      "loss": 0.1632,
      "step": 776
    },
    {
      "epoch": 7.48,
      "grad_norm": 128749.046875,
      "learning_rate": 1.8444e-05,
      "loss": 0.1877,
      "step": 778
    },
    {
      "epoch": 7.5,
      "grad_norm": 180163.609375,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 0.1004,
      "step": 780
    },
    {
      "epoch": 7.52,
      "grad_norm": 270078.53125,
      "learning_rate": 1.8436e-05,
      "loss": 0.1,
      "step": 782
    },
    {
      "epoch": 7.54,
      "grad_norm": 362236.5,
      "learning_rate": 1.8432000000000002e-05,
      "loss": 0.1396,
      "step": 784
    },
    {
      "epoch": 7.56,
      "grad_norm": 98351.84375,
      "learning_rate": 1.8428e-05,
      "loss": 0.3365,
      "step": 786
    },
    {
      "epoch": 7.58,
      "grad_norm": 117782.0078125,
      "learning_rate": 1.8424000000000002e-05,
      "loss": 0.1131,
      "step": 788
    },
    {
      "epoch": 7.6,
      "grad_norm": 233179.984375,
      "learning_rate": 1.8420000000000003e-05,
      "loss": 0.175,
      "step": 790
    },
    {
      "epoch": 7.62,
      "grad_norm": 76694.453125,
      "learning_rate": 1.8416e-05,
      "loss": 0.0973,
      "step": 792
    },
    {
      "epoch": 7.63,
      "grad_norm": 556417.5625,
      "learning_rate": 1.8412000000000003e-05,
      "loss": 0.2237,
      "step": 794
    },
    {
      "epoch": 7.65,
      "grad_norm": 65379.30078125,
      "learning_rate": 1.8408e-05,
      "loss": 0.0865,
      "step": 796
    },
    {
      "epoch": 7.67,
      "grad_norm": 201273.75,
      "learning_rate": 1.8404000000000002e-05,
      "loss": 0.1561,
      "step": 798
    },
    {
      "epoch": 7.69,
      "grad_norm": 106732.0390625,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 0.12,
      "step": 800
    },
    {
      "epoch": 7.69,
      "eval_loss": 0.2601553499698639,
      "eval_runtime": 16.9833,
      "eval_samples_per_second": 13.896,
      "eval_steps_per_second": 1.766,
      "step": 800
    },
    {
      "epoch": 7.71,
      "grad_norm": 513347.34375,
      "learning_rate": 1.8396e-05,
      "loss": 0.1728,
      "step": 802
    },
    {
      "epoch": 7.73,
      "grad_norm": 563029.625,
      "learning_rate": 1.8392e-05,
      "loss": 0.1599,
      "step": 804
    },
    {
      "epoch": 7.75,
      "grad_norm": 339708.1875,
      "learning_rate": 1.8388e-05,
      "loss": 0.2065,
      "step": 806
    },
    {
      "epoch": 7.77,
      "grad_norm": 167385.484375,
      "learning_rate": 1.8384000000000002e-05,
      "loss": 0.128,
      "step": 808
    },
    {
      "epoch": 7.79,
      "grad_norm": 277970.96875,
      "learning_rate": 1.8380000000000004e-05,
      "loss": 0.148,
      "step": 810
    },
    {
      "epoch": 7.81,
      "grad_norm": 442838.09375,
      "learning_rate": 1.8376e-05,
      "loss": 0.1883,
      "step": 812
    },
    {
      "epoch": 7.83,
      "grad_norm": 250161.046875,
      "learning_rate": 1.8372e-05,
      "loss": 0.1282,
      "step": 814
    },
    {
      "epoch": 7.85,
      "grad_norm": 106410.484375,
      "learning_rate": 1.8368e-05,
      "loss": 0.1223,
      "step": 816
    },
    {
      "epoch": 7.87,
      "grad_norm": 117633.40625,
      "learning_rate": 1.8364000000000002e-05,
      "loss": 0.0942,
      "step": 818
    },
    {
      "epoch": 7.88,
      "grad_norm": 61080.54296875,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 0.084,
      "step": 820
    },
    {
      "epoch": 7.9,
      "grad_norm": 114991.8984375,
      "learning_rate": 1.8356000000000002e-05,
      "loss": 0.1205,
      "step": 822
    },
    {
      "epoch": 7.92,
      "grad_norm": 67235.9765625,
      "learning_rate": 1.8352e-05,
      "loss": 0.1014,
      "step": 824
    },
    {
      "epoch": 7.94,
      "grad_norm": 409067.625,
      "learning_rate": 1.8348e-05,
      "loss": 0.1991,
      "step": 826
    },
    {
      "epoch": 7.96,
      "grad_norm": 171954.671875,
      "learning_rate": 1.8344000000000003e-05,
      "loss": 0.1435,
      "step": 828
    },
    {
      "epoch": 7.98,
      "grad_norm": 82680.6328125,
      "learning_rate": 1.834e-05,
      "loss": 0.0914,
      "step": 830
    },
    {
      "epoch": 8.0,
      "grad_norm": 289052.21875,
      "learning_rate": 1.8336000000000002e-05,
      "loss": 0.1395,
      "step": 832
    },
    {
      "epoch": 8.02,
      "grad_norm": 198570.453125,
      "learning_rate": 1.8332e-05,
      "loss": 0.1153,
      "step": 834
    },
    {
      "epoch": 8.04,
      "grad_norm": 133018.234375,
      "learning_rate": 1.8328e-05,
      "loss": 0.1147,
      "step": 836
    },
    {
      "epoch": 8.06,
      "grad_norm": 120844.2734375,
      "learning_rate": 1.8324000000000003e-05,
      "loss": 0.118,
      "step": 838
    },
    {
      "epoch": 8.08,
      "grad_norm": 99584.3046875,
      "learning_rate": 1.832e-05,
      "loss": 0.0858,
      "step": 840
    },
    {
      "epoch": 8.1,
      "grad_norm": 119648.5234375,
      "learning_rate": 1.8316e-05,
      "loss": 0.2463,
      "step": 842
    },
    {
      "epoch": 8.12,
      "grad_norm": 112398.1328125,
      "learning_rate": 1.8312e-05,
      "loss": 0.0793,
      "step": 844
    },
    {
      "epoch": 8.13,
      "grad_norm": 526247.1875,
      "learning_rate": 1.8308e-05,
      "loss": 0.2759,
      "step": 846
    },
    {
      "epoch": 8.15,
      "grad_norm": 151271.15625,
      "learning_rate": 1.8304000000000003e-05,
      "loss": 0.0927,
      "step": 848
    },
    {
      "epoch": 8.17,
      "grad_norm": 71179.78125,
      "learning_rate": 1.83e-05,
      "loss": 0.1575,
      "step": 850
    },
    {
      "epoch": 8.19,
      "grad_norm": 334556.875,
      "learning_rate": 1.8296e-05,
      "loss": 0.1189,
      "step": 852
    },
    {
      "epoch": 8.21,
      "grad_norm": 141665.484375,
      "learning_rate": 1.8292e-05,
      "loss": 0.131,
      "step": 854
    },
    {
      "epoch": 8.23,
      "grad_norm": 138136.984375,
      "learning_rate": 1.8288000000000002e-05,
      "loss": 0.2224,
      "step": 856
    },
    {
      "epoch": 8.25,
      "grad_norm": 161125.0,
      "learning_rate": 1.8284000000000003e-05,
      "loss": 0.1169,
      "step": 858
    },
    {
      "epoch": 8.27,
      "grad_norm": 93900.3828125,
      "learning_rate": 1.828e-05,
      "loss": 0.1123,
      "step": 860
    },
    {
      "epoch": 8.29,
      "grad_norm": 164889.015625,
      "learning_rate": 1.8276e-05,
      "loss": 0.1132,
      "step": 862
    },
    {
      "epoch": 8.31,
      "grad_norm": 143603.453125,
      "learning_rate": 1.8272e-05,
      "loss": 0.0792,
      "step": 864
    },
    {
      "epoch": 8.33,
      "grad_norm": 175501.625,
      "learning_rate": 1.8268000000000002e-05,
      "loss": 0.0874,
      "step": 866
    },
    {
      "epoch": 8.35,
      "grad_norm": 147359.515625,
      "learning_rate": 1.8264000000000003e-05,
      "loss": 0.1538,
      "step": 868
    },
    {
      "epoch": 8.37,
      "grad_norm": 222101.71875,
      "learning_rate": 1.826e-05,
      "loss": 0.1463,
      "step": 870
    },
    {
      "epoch": 8.38,
      "grad_norm": 140229.9375,
      "learning_rate": 1.8256e-05,
      "loss": 0.1155,
      "step": 872
    },
    {
      "epoch": 8.4,
      "grad_norm": 118322.8359375,
      "learning_rate": 1.8252e-05,
      "loss": 0.1032,
      "step": 874
    },
    {
      "epoch": 8.42,
      "grad_norm": 530588.9375,
      "learning_rate": 1.8248000000000002e-05,
      "loss": 0.1241,
      "step": 876
    },
    {
      "epoch": 8.44,
      "grad_norm": 120208.0703125,
      "learning_rate": 1.8244e-05,
      "loss": 0.1418,
      "step": 878
    },
    {
      "epoch": 8.46,
      "grad_norm": 358776.71875,
      "learning_rate": 1.824e-05,
      "loss": 0.1079,
      "step": 880
    },
    {
      "epoch": 8.48,
      "grad_norm": 52452.87890625,
      "learning_rate": 1.8236000000000003e-05,
      "loss": 0.0674,
      "step": 882
    },
    {
      "epoch": 8.5,
      "grad_norm": 161422.90625,
      "learning_rate": 1.8232e-05,
      "loss": 0.0841,
      "step": 884
    },
    {
      "epoch": 8.52,
      "grad_norm": 130289.0859375,
      "learning_rate": 1.8228000000000002e-05,
      "loss": 0.0993,
      "step": 886
    },
    {
      "epoch": 8.54,
      "grad_norm": 111617.7890625,
      "learning_rate": 1.8224e-05,
      "loss": 0.1477,
      "step": 888
    },
    {
      "epoch": 8.56,
      "grad_norm": 589531.125,
      "learning_rate": 1.8220000000000002e-05,
      "loss": 0.1217,
      "step": 890
    },
    {
      "epoch": 8.58,
      "grad_norm": 374200.78125,
      "learning_rate": 1.8216000000000003e-05,
      "loss": 0.2207,
      "step": 892
    },
    {
      "epoch": 8.6,
      "grad_norm": 123896.1875,
      "learning_rate": 1.8212e-05,
      "loss": 0.0986,
      "step": 894
    },
    {
      "epoch": 8.62,
      "grad_norm": 190780.890625,
      "learning_rate": 1.8208000000000003e-05,
      "loss": 0.1118,
      "step": 896
    },
    {
      "epoch": 8.63,
      "grad_norm": 160472.421875,
      "learning_rate": 1.8204e-05,
      "loss": 0.0973,
      "step": 898
    },
    {
      "epoch": 8.65,
      "grad_norm": 205429.15625,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.1136,
      "step": 900
    },
    {
      "epoch": 8.67,
      "grad_norm": 246220.859375,
      "learning_rate": 1.8196000000000003e-05,
      "loss": 0.1535,
      "step": 902
    },
    {
      "epoch": 8.69,
      "grad_norm": 114941.953125,
      "learning_rate": 1.8192e-05,
      "loss": 0.122,
      "step": 904
    },
    {
      "epoch": 8.71,
      "grad_norm": 392744.4375,
      "learning_rate": 1.8188000000000003e-05,
      "loss": 0.1459,
      "step": 906
    },
    {
      "epoch": 8.73,
      "grad_norm": 207950.3125,
      "learning_rate": 1.8184e-05,
      "loss": 0.1499,
      "step": 908
    },
    {
      "epoch": 8.75,
      "grad_norm": 118353.4296875,
      "learning_rate": 1.8180000000000002e-05,
      "loss": 0.1252,
      "step": 910
    },
    {
      "epoch": 8.77,
      "grad_norm": 177779.953125,
      "learning_rate": 1.8176000000000004e-05,
      "loss": 0.0995,
      "step": 912
    },
    {
      "epoch": 8.79,
      "grad_norm": 172347.609375,
      "learning_rate": 1.8172e-05,
      "loss": 0.1409,
      "step": 914
    },
    {
      "epoch": 8.81,
      "grad_norm": 208513.078125,
      "learning_rate": 1.8168e-05,
      "loss": 0.0918,
      "step": 916
    },
    {
      "epoch": 8.83,
      "grad_norm": 194715.5625,
      "learning_rate": 1.8164e-05,
      "loss": 0.1417,
      "step": 918
    },
    {
      "epoch": 8.85,
      "grad_norm": 111518.5390625,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 0.1245,
      "step": 920
    },
    {
      "epoch": 8.87,
      "grad_norm": 878690.6875,
      "learning_rate": 1.8156000000000004e-05,
      "loss": 0.1488,
      "step": 922
    },
    {
      "epoch": 8.88,
      "grad_norm": 245880.234375,
      "learning_rate": 1.8152000000000002e-05,
      "loss": 0.121,
      "step": 924
    },
    {
      "epoch": 8.9,
      "grad_norm": 204389.5,
      "learning_rate": 1.8148e-05,
      "loss": 0.1072,
      "step": 926
    },
    {
      "epoch": 8.92,
      "grad_norm": 95849.5,
      "learning_rate": 1.8144e-05,
      "loss": 0.0956,
      "step": 928
    },
    {
      "epoch": 8.94,
      "grad_norm": 121807.2265625,
      "learning_rate": 1.8140000000000003e-05,
      "loss": 0.1337,
      "step": 930
    },
    {
      "epoch": 8.96,
      "grad_norm": 120121.7265625,
      "learning_rate": 1.8136000000000004e-05,
      "loss": 0.1334,
      "step": 932
    },
    {
      "epoch": 8.98,
      "grad_norm": 157199.234375,
      "learning_rate": 1.8132000000000002e-05,
      "loss": 0.1457,
      "step": 934
    },
    {
      "epoch": 9.0,
      "grad_norm": 294616.65625,
      "learning_rate": 1.8128e-05,
      "loss": 0.0992,
      "step": 936
    },
    {
      "epoch": 9.02,
      "grad_norm": 163483.6875,
      "learning_rate": 1.8124e-05,
      "loss": 0.1359,
      "step": 938
    },
    {
      "epoch": 9.04,
      "grad_norm": 281866.15625,
      "learning_rate": 1.8120000000000003e-05,
      "loss": 0.1499,
      "step": 940
    },
    {
      "epoch": 9.06,
      "grad_norm": 119084.671875,
      "learning_rate": 1.8116000000000004e-05,
      "loss": 0.1548,
      "step": 942
    },
    {
      "epoch": 9.08,
      "grad_norm": 214641.3125,
      "learning_rate": 1.8112000000000002e-05,
      "loss": 0.1608,
      "step": 944
    },
    {
      "epoch": 9.1,
      "grad_norm": 161591.65625,
      "learning_rate": 1.8108e-05,
      "loss": 0.1178,
      "step": 946
    },
    {
      "epoch": 9.12,
      "grad_norm": 134159.375,
      "learning_rate": 1.8104e-05,
      "loss": 0.1084,
      "step": 948
    },
    {
      "epoch": 9.13,
      "grad_norm": 157487.546875,
      "learning_rate": 1.8100000000000003e-05,
      "loss": 0.1069,
      "step": 950
    },
    {
      "epoch": 9.15,
      "grad_norm": 92604.5703125,
      "learning_rate": 1.8096e-05,
      "loss": 0.1311,
      "step": 952
    },
    {
      "epoch": 9.17,
      "grad_norm": 191766.25,
      "learning_rate": 1.8092000000000002e-05,
      "loss": 0.108,
      "step": 954
    },
    {
      "epoch": 9.19,
      "grad_norm": 145217.890625,
      "learning_rate": 1.8088e-05,
      "loss": 0.1031,
      "step": 956
    },
    {
      "epoch": 9.21,
      "grad_norm": 666781.6875,
      "learning_rate": 1.8084e-05,
      "loss": 0.2751,
      "step": 958
    },
    {
      "epoch": 9.23,
      "grad_norm": 278143.28125,
      "learning_rate": 1.8080000000000003e-05,
      "loss": 0.1123,
      "step": 960
    },
    {
      "epoch": 9.25,
      "grad_norm": 359134.875,
      "learning_rate": 1.8076e-05,
      "loss": 0.1673,
      "step": 962
    },
    {
      "epoch": 9.27,
      "grad_norm": 499122.78125,
      "learning_rate": 1.8072e-05,
      "loss": 0.0914,
      "step": 964
    },
    {
      "epoch": 9.29,
      "grad_norm": 175370.546875,
      "learning_rate": 1.8068e-05,
      "loss": 0.1402,
      "step": 966
    },
    {
      "epoch": 9.31,
      "grad_norm": 211093.328125,
      "learning_rate": 1.8064000000000002e-05,
      "loss": 0.1334,
      "step": 968
    },
    {
      "epoch": 9.33,
      "grad_norm": 106434.4453125,
      "learning_rate": 1.8060000000000003e-05,
      "loss": 0.1292,
      "step": 970
    },
    {
      "epoch": 9.35,
      "grad_norm": 272224.40625,
      "learning_rate": 1.8056e-05,
      "loss": 0.1829,
      "step": 972
    },
    {
      "epoch": 9.37,
      "grad_norm": 274613.25,
      "learning_rate": 1.8052e-05,
      "loss": 0.1327,
      "step": 974
    },
    {
      "epoch": 9.38,
      "grad_norm": 73512.0234375,
      "learning_rate": 1.8048e-05,
      "loss": 0.0993,
      "step": 976
    },
    {
      "epoch": 9.4,
      "grad_norm": 116883.984375,
      "learning_rate": 1.8044000000000002e-05,
      "loss": 0.0875,
      "step": 978
    },
    {
      "epoch": 9.42,
      "grad_norm": 196983.65625,
      "learning_rate": 1.8040000000000003e-05,
      "loss": 0.1033,
      "step": 980
    },
    {
      "epoch": 9.44,
      "grad_norm": 340130.15625,
      "learning_rate": 1.8036e-05,
      "loss": 0.0965,
      "step": 982
    },
    {
      "epoch": 9.46,
      "grad_norm": 454953.625,
      "learning_rate": 1.8032e-05,
      "loss": 0.1494,
      "step": 984
    },
    {
      "epoch": 9.48,
      "grad_norm": 396758.09375,
      "learning_rate": 1.8028e-05,
      "loss": 0.1501,
      "step": 986
    },
    {
      "epoch": 9.5,
      "grad_norm": 276095.15625,
      "learning_rate": 1.8024000000000002e-05,
      "loss": 0.1097,
      "step": 988
    },
    {
      "epoch": 9.52,
      "grad_norm": 141050.421875,
      "learning_rate": 1.802e-05,
      "loss": 0.0889,
      "step": 990
    },
    {
      "epoch": 9.54,
      "grad_norm": 154216.875,
      "learning_rate": 1.8016e-05,
      "loss": 0.2242,
      "step": 992
    },
    {
      "epoch": 9.56,
      "grad_norm": 333715.5625,
      "learning_rate": 1.8012e-05,
      "loss": 0.1247,
      "step": 994
    },
    {
      "epoch": 9.58,
      "grad_norm": 80735.8828125,
      "learning_rate": 1.8008e-05,
      "loss": 0.0933,
      "step": 996
    },
    {
      "epoch": 9.6,
      "grad_norm": 108936.453125,
      "learning_rate": 1.8004000000000002e-05,
      "loss": 0.0939,
      "step": 998
    },
    {
      "epoch": 9.62,
      "grad_norm": 177522.21875,
      "learning_rate": 1.8e-05,
      "loss": 0.1145,
      "step": 1000
    },
    {
      "epoch": 9.62,
      "eval_loss": 0.2340807020664215,
      "eval_runtime": 15.8721,
      "eval_samples_per_second": 14.869,
      "eval_steps_per_second": 1.89,
      "step": 1000
    },
    {
      "epoch": 9.63,
      "grad_norm": 153598.65625,
      "learning_rate": 1.7996000000000002e-05,
      "loss": 0.1156,
      "step": 1002
    },
    {
      "epoch": 9.65,
      "grad_norm": 278169.75,
      "learning_rate": 1.7992e-05,
      "loss": 0.2152,
      "step": 1004
    },
    {
      "epoch": 9.67,
      "grad_norm": 201803.1875,
      "learning_rate": 1.7988e-05,
      "loss": 0.1271,
      "step": 1006
    },
    {
      "epoch": 9.69,
      "grad_norm": 69197.6953125,
      "learning_rate": 1.7984000000000003e-05,
      "loss": 0.0742,
      "step": 1008
    },
    {
      "epoch": 9.71,
      "grad_norm": 162236.6875,
      "learning_rate": 1.798e-05,
      "loss": 0.2606,
      "step": 1010
    },
    {
      "epoch": 9.73,
      "grad_norm": 233302.59375,
      "learning_rate": 1.7976000000000002e-05,
      "loss": 0.1105,
      "step": 1012
    },
    {
      "epoch": 9.75,
      "grad_norm": 272443.96875,
      "learning_rate": 1.7972e-05,
      "loss": 0.1013,
      "step": 1014
    },
    {
      "epoch": 9.77,
      "grad_norm": 111935.875,
      "learning_rate": 1.7968e-05,
      "loss": 0.0973,
      "step": 1016
    },
    {
      "epoch": 9.79,
      "grad_norm": 205167.234375,
      "learning_rate": 1.7964000000000003e-05,
      "loss": 0.1041,
      "step": 1018
    },
    {
      "epoch": 9.81,
      "grad_norm": 440608.625,
      "learning_rate": 1.796e-05,
      "loss": 0.1213,
      "step": 1020
    },
    {
      "epoch": 9.83,
      "grad_norm": 172728.328125,
      "learning_rate": 1.7956000000000002e-05,
      "loss": 0.1405,
      "step": 1022
    },
    {
      "epoch": 9.85,
      "grad_norm": 379199.15625,
      "learning_rate": 1.7952e-05,
      "loss": 0.1191,
      "step": 1024
    },
    {
      "epoch": 9.87,
      "grad_norm": 120594.0390625,
      "learning_rate": 1.7948e-05,
      "loss": 0.0959,
      "step": 1026
    },
    {
      "epoch": 9.88,
      "grad_norm": 69459.109375,
      "learning_rate": 1.7944000000000003e-05,
      "loss": 0.0991,
      "step": 1028
    },
    {
      "epoch": 9.9,
      "grad_norm": 69915.5546875,
      "learning_rate": 1.794e-05,
      "loss": 0.088,
      "step": 1030
    },
    {
      "epoch": 9.92,
      "grad_norm": 87955.796875,
      "learning_rate": 1.7936000000000002e-05,
      "loss": 0.1204,
      "step": 1032
    },
    {
      "epoch": 9.94,
      "grad_norm": 788050.0,
      "learning_rate": 1.7932e-05,
      "loss": 0.0899,
      "step": 1034
    },
    {
      "epoch": 9.96,
      "grad_norm": 206145.140625,
      "learning_rate": 1.7928000000000002e-05,
      "loss": 0.1452,
      "step": 1036
    },
    {
      "epoch": 9.98,
      "grad_norm": 126427.21875,
      "learning_rate": 1.7924e-05,
      "loss": 0.1815,
      "step": 1038
    },
    {
      "epoch": 10.0,
      "grad_norm": 107942.671875,
      "learning_rate": 1.792e-05,
      "loss": 0.1678,
      "step": 1040
    },
    {
      "epoch": 10.02,
      "grad_norm": 105548.515625,
      "learning_rate": 1.7916000000000003e-05,
      "loss": 0.1053,
      "step": 1042
    },
    {
      "epoch": 10.04,
      "grad_norm": 663307.75,
      "learning_rate": 1.7912e-05,
      "loss": 0.1244,
      "step": 1044
    },
    {
      "epoch": 10.06,
      "grad_norm": 121214.4765625,
      "learning_rate": 1.7908000000000002e-05,
      "loss": 0.1214,
      "step": 1046
    },
    {
      "epoch": 10.08,
      "grad_norm": 94156.6953125,
      "learning_rate": 1.7904e-05,
      "loss": 0.1079,
      "step": 1048
    },
    {
      "epoch": 10.1,
      "grad_norm": 182804.078125,
      "learning_rate": 1.79e-05,
      "loss": 0.1619,
      "step": 1050
    },
    {
      "epoch": 10.12,
      "grad_norm": 230292.875,
      "learning_rate": 1.7896000000000003e-05,
      "loss": 0.1268,
      "step": 1052
    },
    {
      "epoch": 10.13,
      "grad_norm": 166872.984375,
      "learning_rate": 1.7892e-05,
      "loss": 0.1338,
      "step": 1054
    },
    {
      "epoch": 10.15,
      "grad_norm": 117490.390625,
      "learning_rate": 1.7888000000000002e-05,
      "loss": 0.1434,
      "step": 1056
    },
    {
      "epoch": 10.17,
      "grad_norm": 183985.40625,
      "learning_rate": 1.7884e-05,
      "loss": 0.1447,
      "step": 1058
    },
    {
      "epoch": 10.19,
      "grad_norm": 431810.375,
      "learning_rate": 1.788e-05,
      "loss": 0.1939,
      "step": 1060
    },
    {
      "epoch": 10.21,
      "grad_norm": 264402.84375,
      "learning_rate": 1.7876000000000003e-05,
      "loss": 0.1169,
      "step": 1062
    },
    {
      "epoch": 10.23,
      "grad_norm": 93228.3203125,
      "learning_rate": 1.7872e-05,
      "loss": 0.1094,
      "step": 1064
    },
    {
      "epoch": 10.25,
      "grad_norm": 127185.3515625,
      "learning_rate": 1.7868000000000002e-05,
      "loss": 0.1214,
      "step": 1066
    },
    {
      "epoch": 10.27,
      "grad_norm": 204054.96875,
      "learning_rate": 1.7864e-05,
      "loss": 0.1241,
      "step": 1068
    },
    {
      "epoch": 10.29,
      "grad_norm": 256994.828125,
      "learning_rate": 1.7860000000000002e-05,
      "loss": 0.0882,
      "step": 1070
    },
    {
      "epoch": 10.31,
      "grad_norm": 145094.203125,
      "learning_rate": 1.7856000000000003e-05,
      "loss": 0.1383,
      "step": 1072
    },
    {
      "epoch": 10.33,
      "grad_norm": 221812.859375,
      "learning_rate": 1.7852e-05,
      "loss": 0.1664,
      "step": 1074
    },
    {
      "epoch": 10.35,
      "grad_norm": 257353.734375,
      "learning_rate": 1.7848e-05,
      "loss": 0.1292,
      "step": 1076
    },
    {
      "epoch": 10.37,
      "grad_norm": 213123.609375,
      "learning_rate": 1.7844e-05,
      "loss": 0.1143,
      "step": 1078
    },
    {
      "epoch": 10.38,
      "grad_norm": 206624.984375,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.1165,
      "step": 1080
    },
    {
      "epoch": 10.4,
      "grad_norm": 94829.3828125,
      "learning_rate": 1.7836000000000003e-05,
      "loss": 0.083,
      "step": 1082
    },
    {
      "epoch": 10.42,
      "grad_norm": 314569.65625,
      "learning_rate": 1.7832e-05,
      "loss": 0.0926,
      "step": 1084
    },
    {
      "epoch": 10.44,
      "grad_norm": 278701.0,
      "learning_rate": 1.7828e-05,
      "loss": 0.084,
      "step": 1086
    },
    {
      "epoch": 10.46,
      "grad_norm": 136896.359375,
      "learning_rate": 1.7824e-05,
      "loss": 0.0989,
      "step": 1088
    },
    {
      "epoch": 10.48,
      "grad_norm": 68192.3671875,
      "learning_rate": 1.7820000000000002e-05,
      "loss": 0.2421,
      "step": 1090
    },
    {
      "epoch": 10.5,
      "grad_norm": 187028.921875,
      "learning_rate": 1.7816000000000004e-05,
      "loss": 0.1572,
      "step": 1092
    },
    {
      "epoch": 10.52,
      "grad_norm": 99766.1796875,
      "learning_rate": 1.7812e-05,
      "loss": 0.1168,
      "step": 1094
    },
    {
      "epoch": 10.54,
      "grad_norm": 107849.0,
      "learning_rate": 1.7808e-05,
      "loss": 0.0979,
      "step": 1096
    },
    {
      "epoch": 10.56,
      "grad_norm": 108726.6796875,
      "learning_rate": 1.7804e-05,
      "loss": 0.0959,
      "step": 1098
    },
    {
      "epoch": 10.58,
      "grad_norm": 242057.65625,
      "learning_rate": 1.7800000000000002e-05,
      "loss": 0.1319,
      "step": 1100
    },
    {
      "epoch": 10.6,
      "grad_norm": 98673.5546875,
      "learning_rate": 1.7796000000000004e-05,
      "loss": 0.1618,
      "step": 1102
    },
    {
      "epoch": 10.62,
      "grad_norm": 342515.21875,
      "learning_rate": 1.7792000000000002e-05,
      "loss": 0.1913,
      "step": 1104
    },
    {
      "epoch": 10.63,
      "grad_norm": 86391.9453125,
      "learning_rate": 1.7788e-05,
      "loss": 0.1038,
      "step": 1106
    },
    {
      "epoch": 10.65,
      "grad_norm": 97777.859375,
      "learning_rate": 1.7784e-05,
      "loss": 0.1291,
      "step": 1108
    },
    {
      "epoch": 10.67,
      "grad_norm": 287789.625,
      "learning_rate": 1.7780000000000003e-05,
      "loss": 0.3182,
      "step": 1110
    },
    {
      "epoch": 10.69,
      "grad_norm": 88903.4453125,
      "learning_rate": 1.7776e-05,
      "loss": 0.098,
      "step": 1112
    },
    {
      "epoch": 10.71,
      "grad_norm": 222145.71875,
      "learning_rate": 1.7772000000000002e-05,
      "loss": 0.1237,
      "step": 1114
    },
    {
      "epoch": 10.73,
      "grad_norm": 153157.203125,
      "learning_rate": 1.7768e-05,
      "loss": 0.0891,
      "step": 1116
    },
    {
      "epoch": 10.75,
      "grad_norm": 122050.078125,
      "learning_rate": 1.7764e-05,
      "loss": 0.1434,
      "step": 1118
    },
    {
      "epoch": 10.77,
      "grad_norm": 127176.75,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.1511,
      "step": 1120
    },
    {
      "epoch": 10.79,
      "grad_norm": 157172.90625,
      "learning_rate": 1.7756e-05,
      "loss": 0.0866,
      "step": 1122
    },
    {
      "epoch": 10.81,
      "grad_norm": 166503.4375,
      "learning_rate": 1.7752e-05,
      "loss": 0.1434,
      "step": 1124
    },
    {
      "epoch": 10.83,
      "grad_norm": 112464.1484375,
      "learning_rate": 1.7748e-05,
      "loss": 0.0792,
      "step": 1126
    },
    {
      "epoch": 10.85,
      "grad_norm": 264283.40625,
      "learning_rate": 1.7744e-05,
      "loss": 0.095,
      "step": 1128
    },
    {
      "epoch": 10.87,
      "grad_norm": 210723.859375,
      "learning_rate": 1.7740000000000003e-05,
      "loss": 0.1264,
      "step": 1130
    },
    {
      "epoch": 10.88,
      "grad_norm": 176770.828125,
      "learning_rate": 1.7736e-05,
      "loss": 0.1895,
      "step": 1132
    },
    {
      "epoch": 10.9,
      "grad_norm": 42255.75,
      "learning_rate": 1.7732000000000002e-05,
      "loss": 0.0849,
      "step": 1134
    },
    {
      "epoch": 10.92,
      "grad_norm": 65557.5078125,
      "learning_rate": 1.7728e-05,
      "loss": 0.1379,
      "step": 1136
    },
    {
      "epoch": 10.94,
      "grad_norm": 93687.0546875,
      "learning_rate": 1.7724000000000002e-05,
      "loss": 0.1197,
      "step": 1138
    },
    {
      "epoch": 10.96,
      "grad_norm": 172634.25,
      "learning_rate": 1.7720000000000003e-05,
      "loss": 0.1236,
      "step": 1140
    },
    {
      "epoch": 10.98,
      "grad_norm": 148400.15625,
      "learning_rate": 1.7716e-05,
      "loss": 0.1194,
      "step": 1142
    },
    {
      "epoch": 11.0,
      "grad_norm": 455898.625,
      "learning_rate": 1.7712000000000003e-05,
      "loss": 0.1532,
      "step": 1144
    },
    {
      "epoch": 11.02,
      "grad_norm": 225399.09375,
      "learning_rate": 1.7708e-05,
      "loss": 0.1237,
      "step": 1146
    },
    {
      "epoch": 11.04,
      "grad_norm": 163586.453125,
      "learning_rate": 1.7704000000000002e-05,
      "loss": 0.0979,
      "step": 1148
    },
    {
      "epoch": 11.06,
      "grad_norm": 188382.859375,
      "learning_rate": 1.77e-05,
      "loss": 0.1326,
      "step": 1150
    },
    {
      "epoch": 11.08,
      "grad_norm": 331358.125,
      "learning_rate": 1.7696e-05,
      "loss": 0.1631,
      "step": 1152
    },
    {
      "epoch": 11.1,
      "grad_norm": 127867.328125,
      "learning_rate": 1.7692000000000003e-05,
      "loss": 0.0869,
      "step": 1154
    },
    {
      "epoch": 11.12,
      "grad_norm": 139936.375,
      "learning_rate": 1.7688e-05,
      "loss": 0.1147,
      "step": 1156
    },
    {
      "epoch": 11.13,
      "grad_norm": 71188.7890625,
      "learning_rate": 1.7684000000000002e-05,
      "loss": 0.0963,
      "step": 1158
    },
    {
      "epoch": 11.15,
      "grad_norm": 419405.3125,
      "learning_rate": 1.768e-05,
      "loss": 0.1247,
      "step": 1160
    },
    {
      "epoch": 11.17,
      "grad_norm": 295846.3125,
      "learning_rate": 1.7676e-05,
      "loss": 0.1255,
      "step": 1162
    },
    {
      "epoch": 11.19,
      "grad_norm": 162075.953125,
      "learning_rate": 1.7672000000000003e-05,
      "loss": 0.1226,
      "step": 1164
    },
    {
      "epoch": 11.21,
      "grad_norm": 302422.28125,
      "learning_rate": 1.7668e-05,
      "loss": 0.2453,
      "step": 1166
    },
    {
      "epoch": 11.23,
      "grad_norm": 194218.203125,
      "learning_rate": 1.7664000000000002e-05,
      "loss": 0.1192,
      "step": 1168
    },
    {
      "epoch": 11.25,
      "grad_norm": 148498.609375,
      "learning_rate": 1.766e-05,
      "loss": 0.1164,
      "step": 1170
    },
    {
      "epoch": 11.27,
      "grad_norm": 145975.203125,
      "learning_rate": 1.7656000000000002e-05,
      "loss": 0.1064,
      "step": 1172
    },
    {
      "epoch": 11.29,
      "grad_norm": 181041.984375,
      "learning_rate": 1.7652000000000003e-05,
      "loss": 0.1347,
      "step": 1174
    },
    {
      "epoch": 11.31,
      "grad_norm": 160281.5,
      "learning_rate": 1.7648e-05,
      "loss": 0.111,
      "step": 1176
    },
    {
      "epoch": 11.33,
      "grad_norm": 78471.6015625,
      "learning_rate": 1.7644000000000003e-05,
      "loss": 0.1044,
      "step": 1178
    },
    {
      "epoch": 11.35,
      "grad_norm": 130052.703125,
      "learning_rate": 1.764e-05,
      "loss": 0.1222,
      "step": 1180
    },
    {
      "epoch": 11.37,
      "grad_norm": 127300.9296875,
      "learning_rate": 1.7636000000000002e-05,
      "loss": 0.0812,
      "step": 1182
    },
    {
      "epoch": 11.38,
      "grad_norm": 226585.875,
      "learning_rate": 1.7632000000000003e-05,
      "loss": 0.1507,
      "step": 1184
    },
    {
      "epoch": 11.4,
      "grad_norm": 121615.8359375,
      "learning_rate": 1.7628e-05,
      "loss": 0.1104,
      "step": 1186
    },
    {
      "epoch": 11.42,
      "grad_norm": 223505.6875,
      "learning_rate": 1.7624000000000003e-05,
      "loss": 0.1259,
      "step": 1188
    },
    {
      "epoch": 11.44,
      "grad_norm": 149664.90625,
      "learning_rate": 1.762e-05,
      "loss": 0.1063,
      "step": 1190
    },
    {
      "epoch": 11.46,
      "grad_norm": 78401.1640625,
      "learning_rate": 1.7616000000000002e-05,
      "loss": 0.105,
      "step": 1192
    },
    {
      "epoch": 11.48,
      "grad_norm": 171298.40625,
      "learning_rate": 1.7612000000000003e-05,
      "loss": 0.1034,
      "step": 1194
    },
    {
      "epoch": 11.5,
      "grad_norm": 126314.2578125,
      "learning_rate": 1.7608e-05,
      "loss": 0.1168,
      "step": 1196
    },
    {
      "epoch": 11.52,
      "grad_norm": 44360.03125,
      "learning_rate": 1.7604e-05,
      "loss": 0.0749,
      "step": 1198
    },
    {
      "epoch": 11.54,
      "grad_norm": 281878.53125,
      "learning_rate": 1.76e-05,
      "loss": 0.1033,
      "step": 1200
    },
    {
      "epoch": 11.54,
      "eval_loss": 0.22471846640110016,
      "eval_runtime": 15.748,
      "eval_samples_per_second": 14.986,
      "eval_steps_per_second": 1.905,
      "step": 1200
    },
    {
      "epoch": 11.56,
      "grad_norm": 245144.09375,
      "learning_rate": 1.7596000000000002e-05,
      "loss": 0.1322,
      "step": 1202
    },
    {
      "epoch": 11.58,
      "grad_norm": 93348.65625,
      "learning_rate": 1.7592000000000004e-05,
      "loss": 0.0895,
      "step": 1204
    },
    {
      "epoch": 11.6,
      "grad_norm": 80426.1875,
      "learning_rate": 1.7588e-05,
      "loss": 0.1106,
      "step": 1206
    },
    {
      "epoch": 11.62,
      "grad_norm": 161800.625,
      "learning_rate": 1.7584e-05,
      "loss": 0.1377,
      "step": 1208
    },
    {
      "epoch": 11.63,
      "grad_norm": 79377.2265625,
      "learning_rate": 1.758e-05,
      "loss": 0.0835,
      "step": 1210
    },
    {
      "epoch": 11.65,
      "grad_norm": 77673.2734375,
      "learning_rate": 1.7576000000000002e-05,
      "loss": 0.0891,
      "step": 1212
    },
    {
      "epoch": 11.67,
      "grad_norm": 77021.03125,
      "learning_rate": 1.7572000000000004e-05,
      "loss": 0.0791,
      "step": 1214
    },
    {
      "epoch": 11.69,
      "grad_norm": 424646.78125,
      "learning_rate": 1.7568000000000002e-05,
      "loss": 0.1312,
      "step": 1216
    },
    {
      "epoch": 11.71,
      "grad_norm": 84842.8359375,
      "learning_rate": 1.7564e-05,
      "loss": 0.1134,
      "step": 1218
    },
    {
      "epoch": 11.73,
      "grad_norm": 163751.421875,
      "learning_rate": 1.756e-05,
      "loss": 0.1581,
      "step": 1220
    },
    {
      "epoch": 11.75,
      "grad_norm": 103447.359375,
      "learning_rate": 1.7556000000000003e-05,
      "loss": 0.1069,
      "step": 1222
    },
    {
      "epoch": 11.77,
      "grad_norm": 178463.140625,
      "learning_rate": 1.7552e-05,
      "loss": 0.1288,
      "step": 1224
    },
    {
      "epoch": 11.79,
      "grad_norm": 191417.359375,
      "learning_rate": 1.7548000000000002e-05,
      "loss": 0.1788,
      "step": 1226
    },
    {
      "epoch": 11.81,
      "grad_norm": 54879.85546875,
      "learning_rate": 1.7544e-05,
      "loss": 0.1091,
      "step": 1228
    },
    {
      "epoch": 11.83,
      "grad_norm": 167492.6875,
      "learning_rate": 1.754e-05,
      "loss": 0.1107,
      "step": 1230
    },
    {
      "epoch": 11.85,
      "grad_norm": 289922.34375,
      "learning_rate": 1.7536000000000003e-05,
      "loss": 0.1067,
      "step": 1232
    },
    {
      "epoch": 11.87,
      "grad_norm": 95573.5390625,
      "learning_rate": 1.7532e-05,
      "loss": 0.1135,
      "step": 1234
    },
    {
      "epoch": 11.88,
      "grad_norm": 176271.90625,
      "learning_rate": 1.7528e-05,
      "loss": 0.091,
      "step": 1236
    },
    {
      "epoch": 11.9,
      "grad_norm": 98074.296875,
      "learning_rate": 1.7524e-05,
      "loss": 0.1331,
      "step": 1238
    },
    {
      "epoch": 11.92,
      "grad_norm": 105162.9140625,
      "learning_rate": 1.752e-05,
      "loss": 0.105,
      "step": 1240
    },
    {
      "epoch": 11.94,
      "grad_norm": 110482.1953125,
      "learning_rate": 1.7516000000000003e-05,
      "loss": 0.1367,
      "step": 1242
    },
    {
      "epoch": 11.96,
      "grad_norm": 169151.25,
      "learning_rate": 1.7512e-05,
      "loss": 0.1611,
      "step": 1244
    },
    {
      "epoch": 11.98,
      "grad_norm": 92463.1796875,
      "learning_rate": 1.7508e-05,
      "loss": 0.0984,
      "step": 1246
    },
    {
      "epoch": 12.0,
      "grad_norm": 92133.75,
      "learning_rate": 1.7504e-05,
      "loss": 0.072,
      "step": 1248
    },
    {
      "epoch": 12.02,
      "grad_norm": 195687.234375,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 0.0971,
      "step": 1250
    },
    {
      "epoch": 12.04,
      "grad_norm": 116531.984375,
      "learning_rate": 1.7496000000000003e-05,
      "loss": 0.1021,
      "step": 1252
    },
    {
      "epoch": 12.06,
      "grad_norm": 124666.4296875,
      "learning_rate": 1.7492e-05,
      "loss": 0.12,
      "step": 1254
    },
    {
      "epoch": 12.08,
      "grad_norm": 257082.421875,
      "learning_rate": 1.7488e-05,
      "loss": 0.1553,
      "step": 1256
    },
    {
      "epoch": 12.1,
      "grad_norm": 134429.03125,
      "learning_rate": 1.7484e-05,
      "loss": 0.1068,
      "step": 1258
    },
    {
      "epoch": 12.12,
      "grad_norm": 131810.671875,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 0.1384,
      "step": 1260
    },
    {
      "epoch": 12.13,
      "grad_norm": 138322.453125,
      "learning_rate": 1.7476000000000003e-05,
      "loss": 0.1336,
      "step": 1262
    },
    {
      "epoch": 12.15,
      "grad_norm": 94911.4375,
      "learning_rate": 1.7472e-05,
      "loss": 0.1074,
      "step": 1264
    },
    {
      "epoch": 12.17,
      "grad_norm": 227515.171875,
      "learning_rate": 1.7468e-05,
      "loss": 0.098,
      "step": 1266
    },
    {
      "epoch": 12.19,
      "grad_norm": 170038.1875,
      "learning_rate": 1.7464e-05,
      "loss": 0.0927,
      "step": 1268
    },
    {
      "epoch": 12.21,
      "grad_norm": 185601.421875,
      "learning_rate": 1.7460000000000002e-05,
      "loss": 0.2749,
      "step": 1270
    },
    {
      "epoch": 12.23,
      "grad_norm": 158753.03125,
      "learning_rate": 1.7456e-05,
      "loss": 0.0935,
      "step": 1272
    },
    {
      "epoch": 12.25,
      "grad_norm": 142602.09375,
      "learning_rate": 1.7452e-05,
      "loss": 0.1181,
      "step": 1274
    },
    {
      "epoch": 12.27,
      "grad_norm": 208347.109375,
      "learning_rate": 1.7448e-05,
      "loss": 0.1316,
      "step": 1276
    },
    {
      "epoch": 12.29,
      "grad_norm": 154435.25,
      "learning_rate": 1.7444e-05,
      "loss": 0.1186,
      "step": 1278
    },
    {
      "epoch": 12.31,
      "grad_norm": 202676.890625,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.1074,
      "step": 1280
    },
    {
      "epoch": 12.33,
      "grad_norm": 123114.0234375,
      "learning_rate": 1.7436e-05,
      "loss": 0.1297,
      "step": 1282
    },
    {
      "epoch": 12.35,
      "grad_norm": 141137.953125,
      "learning_rate": 1.7432000000000002e-05,
      "loss": 0.1093,
      "step": 1284
    },
    {
      "epoch": 12.37,
      "grad_norm": 93761.03125,
      "learning_rate": 1.7428e-05,
      "loss": 0.0756,
      "step": 1286
    },
    {
      "epoch": 12.38,
      "grad_norm": 136571.3125,
      "learning_rate": 1.7424e-05,
      "loss": 0.0966,
      "step": 1288
    },
    {
      "epoch": 12.4,
      "grad_norm": 170516.796875,
      "learning_rate": 1.7420000000000003e-05,
      "loss": 0.1245,
      "step": 1290
    },
    {
      "epoch": 12.42,
      "grad_norm": 195582.484375,
      "learning_rate": 1.7416e-05,
      "loss": 0.1628,
      "step": 1292
    },
    {
      "epoch": 12.44,
      "grad_norm": 228179.625,
      "learning_rate": 1.7412000000000002e-05,
      "loss": 0.1193,
      "step": 1294
    },
    {
      "epoch": 12.46,
      "grad_norm": 217078.078125,
      "learning_rate": 1.7408e-05,
      "loss": 0.1729,
      "step": 1296
    },
    {
      "epoch": 12.48,
      "grad_norm": 254617.625,
      "learning_rate": 1.7404e-05,
      "loss": 0.1564,
      "step": 1298
    },
    {
      "epoch": 12.5,
      "grad_norm": 66423.953125,
      "learning_rate": 1.7400000000000003e-05,
      "loss": 0.1037,
      "step": 1300
    },
    {
      "epoch": 12.52,
      "grad_norm": 125045.3671875,
      "learning_rate": 1.7396e-05,
      "loss": 0.1398,
      "step": 1302
    },
    {
      "epoch": 12.54,
      "grad_norm": 181992.515625,
      "learning_rate": 1.7392000000000002e-05,
      "loss": 0.1457,
      "step": 1304
    },
    {
      "epoch": 12.56,
      "grad_norm": 62455.78515625,
      "learning_rate": 1.7388e-05,
      "loss": 0.0955,
      "step": 1306
    },
    {
      "epoch": 12.58,
      "grad_norm": 122553.7421875,
      "learning_rate": 1.7384e-05,
      "loss": 0.0795,
      "step": 1308
    },
    {
      "epoch": 12.6,
      "grad_norm": 118501.765625,
      "learning_rate": 1.7380000000000003e-05,
      "loss": 0.0862,
      "step": 1310
    },
    {
      "epoch": 12.62,
      "grad_norm": 158245.171875,
      "learning_rate": 1.7376e-05,
      "loss": 0.0997,
      "step": 1312
    },
    {
      "epoch": 12.63,
      "grad_norm": 139339.71875,
      "learning_rate": 1.7372000000000002e-05,
      "loss": 0.0959,
      "step": 1314
    },
    {
      "epoch": 12.65,
      "grad_norm": 172260.546875,
      "learning_rate": 1.7368e-05,
      "loss": 0.1153,
      "step": 1316
    },
    {
      "epoch": 12.67,
      "grad_norm": 164113.28125,
      "learning_rate": 1.7364000000000002e-05,
      "loss": 0.1228,
      "step": 1318
    },
    {
      "epoch": 12.69,
      "grad_norm": 71377.7578125,
      "learning_rate": 1.736e-05,
      "loss": 0.112,
      "step": 1320
    },
    {
      "epoch": 12.71,
      "grad_norm": 264945.71875,
      "learning_rate": 1.7356e-05,
      "loss": 0.1053,
      "step": 1322
    },
    {
      "epoch": 12.73,
      "grad_norm": 83654.5859375,
      "learning_rate": 1.7352000000000003e-05,
      "loss": 0.1174,
      "step": 1324
    },
    {
      "epoch": 12.75,
      "grad_norm": 59218.1875,
      "learning_rate": 1.7348e-05,
      "loss": 0.1004,
      "step": 1326
    },
    {
      "epoch": 12.77,
      "grad_norm": 180743.109375,
      "learning_rate": 1.7344000000000002e-05,
      "loss": 0.0903,
      "step": 1328
    },
    {
      "epoch": 12.79,
      "grad_norm": 161848.515625,
      "learning_rate": 1.734e-05,
      "loss": 0.1679,
      "step": 1330
    },
    {
      "epoch": 12.81,
      "grad_norm": 106994.2734375,
      "learning_rate": 1.7336e-05,
      "loss": 0.141,
      "step": 1332
    },
    {
      "epoch": 12.83,
      "grad_norm": 409553.25,
      "learning_rate": 1.7332000000000003e-05,
      "loss": 0.1948,
      "step": 1334
    },
    {
      "epoch": 12.85,
      "grad_norm": 85259.609375,
      "learning_rate": 1.7328e-05,
      "loss": 0.1046,
      "step": 1336
    },
    {
      "epoch": 12.87,
      "grad_norm": 72699.375,
      "learning_rate": 1.7324000000000002e-05,
      "loss": 0.1098,
      "step": 1338
    },
    {
      "epoch": 12.88,
      "grad_norm": 165006.03125,
      "learning_rate": 1.732e-05,
      "loss": 0.1376,
      "step": 1340
    },
    {
      "epoch": 12.9,
      "grad_norm": 188966.0625,
      "learning_rate": 1.7316e-05,
      "loss": 0.1336,
      "step": 1342
    },
    {
      "epoch": 12.92,
      "grad_norm": 129124.1484375,
      "learning_rate": 1.7312000000000003e-05,
      "loss": 0.0813,
      "step": 1344
    },
    {
      "epoch": 12.94,
      "grad_norm": 173979.03125,
      "learning_rate": 1.7308e-05,
      "loss": 0.1143,
      "step": 1346
    },
    {
      "epoch": 12.96,
      "grad_norm": 200324.59375,
      "learning_rate": 1.7304000000000002e-05,
      "loss": 0.0892,
      "step": 1348
    },
    {
      "epoch": 12.98,
      "grad_norm": 502821.5,
      "learning_rate": 1.73e-05,
      "loss": 0.2383,
      "step": 1350
    },
    {
      "epoch": 13.0,
      "grad_norm": 357007.8125,
      "learning_rate": 1.7296000000000002e-05,
      "loss": 0.1858,
      "step": 1352
    },
    {
      "epoch": 13.02,
      "grad_norm": 67692.265625,
      "learning_rate": 1.7292000000000003e-05,
      "loss": 0.0978,
      "step": 1354
    },
    {
      "epoch": 13.04,
      "grad_norm": 86186.875,
      "learning_rate": 1.7288e-05,
      "loss": 0.1673,
      "step": 1356
    },
    {
      "epoch": 13.06,
      "grad_norm": 45355.171875,
      "learning_rate": 1.7284e-05,
      "loss": 0.076,
      "step": 1358
    },
    {
      "epoch": 13.08,
      "grad_norm": 76525.2421875,
      "learning_rate": 1.728e-05,
      "loss": 0.1271,
      "step": 1360
    },
    {
      "epoch": 13.1,
      "grad_norm": 103343.203125,
      "learning_rate": 1.7276000000000002e-05,
      "loss": 0.1228,
      "step": 1362
    },
    {
      "epoch": 13.12,
      "grad_norm": 124912.28125,
      "learning_rate": 1.7272000000000003e-05,
      "loss": 0.0777,
      "step": 1364
    },
    {
      "epoch": 13.13,
      "grad_norm": 337870.3125,
      "learning_rate": 1.7268e-05,
      "loss": 0.1145,
      "step": 1366
    },
    {
      "epoch": 13.15,
      "grad_norm": 75645.71875,
      "learning_rate": 1.7264e-05,
      "loss": 0.1129,
      "step": 1368
    },
    {
      "epoch": 13.17,
      "grad_norm": 112357.84375,
      "learning_rate": 1.726e-05,
      "loss": 0.1149,
      "step": 1370
    },
    {
      "epoch": 13.19,
      "grad_norm": 134102.671875,
      "learning_rate": 1.7256000000000002e-05,
      "loss": 0.1454,
      "step": 1372
    },
    {
      "epoch": 13.21,
      "grad_norm": 59345.12109375,
      "learning_rate": 1.7252000000000004e-05,
      "loss": 0.1107,
      "step": 1374
    },
    {
      "epoch": 13.23,
      "grad_norm": 120442.421875,
      "learning_rate": 1.7248e-05,
      "loss": 0.1039,
      "step": 1376
    },
    {
      "epoch": 13.25,
      "grad_norm": 119290.859375,
      "learning_rate": 1.7244e-05,
      "loss": 0.1045,
      "step": 1378
    },
    {
      "epoch": 13.27,
      "grad_norm": 61558.15234375,
      "learning_rate": 1.724e-05,
      "loss": 0.1012,
      "step": 1380
    },
    {
      "epoch": 13.29,
      "grad_norm": 82420.6484375,
      "learning_rate": 1.7236000000000002e-05,
      "loss": 0.1099,
      "step": 1382
    },
    {
      "epoch": 13.31,
      "grad_norm": 99271.8828125,
      "learning_rate": 1.7232000000000004e-05,
      "loss": 0.0991,
      "step": 1384
    },
    {
      "epoch": 13.33,
      "grad_norm": 247728.875,
      "learning_rate": 1.7228000000000002e-05,
      "loss": 0.1199,
      "step": 1386
    },
    {
      "epoch": 13.35,
      "grad_norm": 1629448.875,
      "learning_rate": 1.7224e-05,
      "loss": 0.2019,
      "step": 1388
    },
    {
      "epoch": 13.37,
      "grad_norm": 202290.15625,
      "learning_rate": 1.722e-05,
      "loss": 0.111,
      "step": 1390
    },
    {
      "epoch": 13.38,
      "grad_norm": 137636.984375,
      "learning_rate": 1.7216000000000003e-05,
      "loss": 0.123,
      "step": 1392
    },
    {
      "epoch": 13.4,
      "grad_norm": 151332.53125,
      "learning_rate": 1.7212e-05,
      "loss": 0.1242,
      "step": 1394
    },
    {
      "epoch": 13.42,
      "grad_norm": 110910.421875,
      "learning_rate": 1.7208000000000002e-05,
      "loss": 0.0824,
      "step": 1396
    },
    {
      "epoch": 13.44,
      "grad_norm": 75052.28125,
      "learning_rate": 1.7204e-05,
      "loss": 0.0878,
      "step": 1398
    },
    {
      "epoch": 13.46,
      "grad_norm": 179090.859375,
      "learning_rate": 1.72e-05,
      "loss": 0.1005,
      "step": 1400
    },
    {
      "epoch": 13.46,
      "eval_loss": 0.22086845338344574,
      "eval_runtime": 16.2797,
      "eval_samples_per_second": 14.497,
      "eval_steps_per_second": 1.843,
      "step": 1400
    }
  ],
  "logging_steps": 2,
  "max_steps": 10000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 97,
  "save_steps": 200,
  "total_flos": 1.650178932629255e+19,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
